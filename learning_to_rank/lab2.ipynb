{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "549d14426afb2109edb71ef6e0223d5b",
     "grade": false,
     "grade_id": "cell-133a4667b3e842fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 2: Learning to Rank <a class=\"anchor\" id=\"toptop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "188d0bd6218df31e0a8795322a1b9912",
     "grade": false,
     "grade_id": "cell-9409dd22f820096c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Submission instructions**:\n",
    "- Only the code `TODO: Implement this!` denotes that these sections are graded. Please do not delete the comment #YOUR CODE HERE.\n",
    "- Vectoriize your code instead of using for-loop when implementing Neural Nets.\n",
    "- The notebook you submit has to have the student ids, separated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "- This will be parsed by a regexp, so please double check your filename.\n",
    "- Only one member of each group has to submit the file to canvas.\n",
    "- Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "- Please do not delete/add new cells. Removing cells can lead to grade deduction. Also do not change the number of parameters in the pre-defined functions.\n",
    "- Note, that you are not allowed to use Google Colab.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Part 1: Offline LTR\n",
    "  - Learn how to implement pointwise, pairwise and listwise algorithms for learning to rank \n",
    "- Part 2: Online LTR\n",
    "  - Implement learning to rank algorithms from historical clicks and online evaluation of ranking algorithms.\n",
    "- Learn their weaknesses & strengths and when each method is suitable. \n",
    "\n",
    "\n",
    "\n",
    "**Files to submit along with the completed notebook**:\n",
    "- `pointwise_regression.json`\n",
    "- `pointwise_classification.json`\n",
    "- `pairwise.json`\n",
    "- `listwise.json`\n",
    "- `biased_model.json'\n",
    "- `unbiased_model.json'\n",
    "\n",
    "\n",
    "---\n",
    "**Recommended Reading**:\n",
    "- Part 1:\n",
    "  - Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. InProceedings of the 22nd international conference on Machine learning, pages 89–96, 2005.\n",
    "  - Christopher J Burges, Robert Ragno, and Quoc V Le. Learning to rank with nonsmooth cost functions. In Advances inneural information processing systems, pages 193–200, 2007\n",
    "  - (Sections 1, 2 and 4) Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010\n",
    "  \n",
    "\n",
    "Additional Resources: \n",
    "- This assignment requires knowledge of [PyTorch](https://pytorch.org/). If you are unfamiliar with PyTorch, you can go over [these series of tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "In the previous assignment, you experimented with retrieval with different ranking functions and in addition, different document representations. \n",
    "\n",
    "This assignment deals directly with learning to rank (LTR). In offline LTR (Part 1), You will learn how to implement methods from the three approaches associated with learning to rank: pointwise, pairwise and listwise. \n",
    "\n",
    "In Part 2, you will learn about online LTR. Instead of using manually judged datasets, in online LTR, we learn from user interactions. You will learn how to simulate clicks using click models, how to learn unbiasedly from historical clicks and how to evaluate different rankers in an online environment using multileaving methods. \n",
    "\n",
    "**Note:**\n",
    "  - The dataset used in this assignment is +100Mb in size. You may need around 2Gb of RAM for running the whole notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a595c150f39970fb6ad72e463fe8b44",
     "grade": false,
     "grade_id": "cell-09127508ac207429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "[Back to top](#toptop)\n",
    "\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "\n",
    " - [Chapter 1: Offline LTR](#o_LTR) (345 points)\n",
    "     - [Section 1: Dataset and Utility](#dataU) \n",
    "     - [Section 2: Pointwtise LTR](#pointwiseLTR) (55 points)\n",
    "     - [Section 3: Pairwise LTR](#pairwiseLTR) (60 points)\n",
    "     - [Section 4: Pairwise Speed-up RankNet](#SpairwiseLTR) (70 points)\n",
    "     - [Section 5: Listwise LTR](#listwiseLTR) (80 points)\n",
    "     - [Section 6: Evaluation](#evaluation1) (70 points)\n",
    " - [Chapter 2: Online LTR](#onLTR) (180 points)\n",
    "     - [Section 1: Clicks Simulation](#clicks) (15 points)\n",
    "     - [Section 2: Counterfactual](#cLTR) (90 points)\n",
    "     - [Section 3: Online Evaluation](#on_eval) (75 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:08.415872Z",
     "start_time": "2021-03-11T23:48:06.746105Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7be29958190a403c77402e97c21c5252",
     "grade": false,
     "grade_id": "cell-b08a635cb01047dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import dataset\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f126271fe03e0c82c752179a1293748",
     "grade": false,
     "grade_id": "cell-ef602d983baa9d90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Chapter 1: Offline LTR <a class=\"anchor\" id=\"o_LTR\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83d5c5098ff7e903a1d4475f78d028be",
     "grade": false,
     "grade_id": "cell-9978e0796016b961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A typical setup of learning to rank involves a feature vector constructed using a query-document pair, and a set of relevance judgements. You are given a set of triples (`query`, `document`, `relevance grade`); where relevance grade is an *ordinal* variable  with  5  grades,  for example: {`perfect`,`excellent`,`good`,`fair`,`bad`),  typically  labeled  by human annotators.  \n",
    "\n",
    "In this assignment, you are already given the feature vector for a given document and query pair. To access these vectors, see the following code cells (note: the dataset will be automatically downloaded & the first time the next cell runs, it will take a while!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62aa687b659ad249d6b6190d4b1f7d9e",
     "grade": false,
     "grade_id": "cell-d60b3e2cd8d41210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1: Data and Utility <a class=\"anchor\" id=\"dataU\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "First let's get familiar with the dataset and some utility methods for our implementations.\n",
    "\n",
    "### Section 1.1 Dataset stats\n",
    "\n",
    "| Split Name | \\# queries | \\# docs | \\# features |\n",
    "| :- | :--: | :--: | :--: |\n",
    "| train | 2735 | 85227 | 501 |\n",
    "| validation | 403 | 12794 | 501 |\n",
    "| test | 949 | 29881 | 501 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:09.981591Z",
     "start_time": "2021-03-11T23:48:08.416870Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11c95b755f0b252276313365c6ff290",
     "grade": false,
     "grade_id": "cell-d4779843ecb42649",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset.download_dataset()\n",
    "data = dataset.get_dataset()\n",
    "# there is only 1 fold for this dataset \n",
    "data = data.get_data_folds()[0]\n",
    "# read in the data\n",
    "data.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:09.997659Z",
     "start_time": "2021-03-11T23:48:09.981591Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8008b140d6012489be5056ec30e90444",
     "grade": false,
     "grade_id": "cell-2a79356db5683374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 501\n",
      "Split: train\n",
      "\tNumber of queries 2735\n",
      "\tNumber of docs 85227\n",
      "Split: validation\n",
      "\tNumber of queries 403\n",
      "\tNumber of docs 12794\n",
      "Split: test\n",
      "\tNumber of queries 949\n",
      "\tNumber of docs 29881\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features: {data.num_features}\")\n",
    "# print some statistics\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"Split: {split}\")\n",
    "    split = getattr(data, split)\n",
    "    print(f\"\\tNumber of queries {split.num_queries()}\")\n",
    "    print(f\"\\tNumber of docs {split.num_docs()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70b764af87765e64827eb896b0ad8643",
     "grade": false,
     "grade_id": "cell-5b034476f52f28bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 1.2 Utility classes/methods\n",
    "\n",
    "The following cells contain code that will be useful for the assigment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.026315Z",
     "start_time": "2021-03-11T23:48:09.999653Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb52800727e7a5fe81c92706c34e6471",
     "grade": false,
     "grade_id": "cell-4ad2f0d8e4f66d37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# these is a useful class to create torch DataLoaders, and can be used during training\n",
    "class LTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert split is not None, \"Invalid split!\"\n",
    "        features, labels = split.feature_matrix, split.label_vector\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.120403Z",
     "start_time": "2021-03-11T23:48:10.026315Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61170cd9d5a02b3f9e23364bf7d46c95",
     "grade": false,
     "grade_id": "cell-6be5d30fd0264dc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 501]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "train_dl = DataLoader(LTRData(data, \"train\"), batch_size=32, shuffle=True)\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "# - (of course, without the break statement!)\n",
    "for (x, y) in train_dl:\n",
    "    print(x.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50bdb8c74b13357983e5f5f435b70115",
     "grade": false,
     "grade_id": "cell-a79c0f58db4af010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`evaluate_model` evaluates a model, on a given split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.136360Z",
     "start_time": "2021-03-11T23:48:10.122398Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ca1e81dd1f55111cda0a04093fd223b",
     "grade": false,
     "grade_id": "cell-b66759e20b89e0b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this function evaluates a model, on a given split\n",
    "def evaluate_model(pred_fn, split, batch_size=256, print_results=False, q_level=False):\n",
    "    dl = DataLoader(LTRData(data, split), batch_size=batch_size)\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    for (x, y) in tqdm(dl, desc=f'Eval ({split})', leave=False):\n",
    "        all_labels.append(y.squeeze().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = pred_fn(x)\n",
    "            all_scores.append(output.squeeze().numpy())\n",
    "            \n",
    "    split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "    }.get(split)   \n",
    "    results = evaluate.evaluate2(np.asarray(all_scores), np.asarray(all_labels), print_results=print_results, q_level=q_level)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.451517Z",
     "start_time": "2021-03-11T23:48:10.138357Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c605f95e2cd732774f1813a69bb8c3fc",
     "grade": false,
     "grade_id": "cell-66bc9b1a832d14d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 81.2932 (20.10293)\n",
      "dcg@03: 3.8638 (3.18356)\n",
      "dcg@05: 5.1686 (3.34264)\n",
      "dcg@10: 8.1918 (4.47827)\n",
      "dcg@20: 12.5091 (5.60844)\n",
      "ndcg: 0.6534 (0.05816)\n",
      "ndcg@03: 0.1409 (0.11979)\n",
      "ndcg@05: 0.1438 (0.09997)\n",
      "ndcg@10: 0.1643 (0.09508)\n",
      "ndcg@20: 0.1895 (0.08506)\n",
      "precision@01: 0.0800 (0.27129)\n",
      "precision@03: 0.1133 (0.18391)\n",
      "precision@05: 0.0880 (0.12106)\n",
      "precision@10: 0.0840 (0.10268)\n",
      "precision@20: 0.0730 (0.07497)\n",
      "recall@01: 0.0027 (0.00966)\n",
      "recall@03: 0.0162 (0.03407)\n",
      "recall@05: 0.0219 (0.03753)\n",
      "recall@10: 0.0379 (0.04529)\n",
      "recall@20: 0.0628 (0.06032)\n",
      "relevant rank: 136.9441 (75.90597)\n",
      "relevant rank per query: 3234.6200 (1639.21815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreeateodora/opt/anaconda3/envs/irenv/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "# function that scores a given feature vector e.g a network\n",
    "net = nn.Linear(501, 1)\n",
    "# the evaluate method accepts a function. more specifically, a callable (such as pytorch modules) \n",
    "def notwork(x):\n",
    "    return net(x)\n",
    "# evaluate the function\n",
    "_ = evaluate_model(notwork, \"validation\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f71c11c5be87af7e7109a463a1e24c6c",
     "grade": false,
     "grade_id": "cell-66ae15ed8cb736b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell is used to generate reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.735757Z",
     "start_time": "2021-03-11T23:48:10.722792Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d81a93ddde3c0ae3be42eba5a6ba025d",
     "grade": false,
     "grade_id": "cell-df3d4a5ebf6dece6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use to get reproducible results\n",
    "def seed(random_seed):\n",
    "    import random\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acd2f7fa9d402a7704d9f7f5fc1c2c89",
     "grade": false,
     "grade_id": "cell-a29483034efce729",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Pointwise LTR (55 points) <a class=\"anchor\" id=\"pointwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be an input feature vector, containing features for a query-document pair. Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ be a function that maps this feature vector to a number $f(x)$ - either a relevance score (regression) or label (classification). The data $\\{x \\}$ are treated as feature vectors and the relevance judgements are treated as the target which we want to predict. \n",
    "\n",
    "In this section, you will implement a simple Pointwise model using either a regression or classification loss, and use the train set to train this model to predict (or classify) the relevance score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0f5f848b2a3509141e384bd4d101923",
     "grade": false,
     "grade_id": "cell-fdcb0b1bd78f6eda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.1: Neural Model (5 points)\n",
    "\n",
    "In the following cell, you will implement a simple pointwise LTR model: \n",
    "- Use a neural network to learn a Pointwise model using both a regression and a classification loss, using the relevance grades as the label. Use the following parameters: \n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow o$ where $o$ is either 5 for classification or 1 for regression, where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLU`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)\n",
    "  - Note: Do not use a `nn.Softmax` function here - it will be taken care of later!\n",
    "  - This network will also be used by other methods i.e Pairwise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.750717Z",
     "start_time": "2021-03-11T23:48:10.737752Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbaeb826028de6d6d7429ee18c90f455",
     "grade": false,
     "grade_id": "cell-e6ebad1d98f78bf0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class NeuralModule(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the Pointwise neural network. \n",
    "        Input: output_dim: The dimension of the output layer. In this assignment, \n",
    "                it is either 1 (regression) or 5 (classification)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_features=501, out_features=256),\n",
    "                                 nn.ReLU(),\n",
    "                                nn.Linear(in_features=256, out_features=output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes in an input feature vector (of size 501) and produces the (regression/classification) output \n",
    "        Input: x: a [N, 501] tensor\n",
    "        Output: a [N, output_dim] tensor\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.766674Z",
     "start_time": "2021-03-11T23:48:10.752712Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eebff5ba2f470a05674e79514a6ba7bc",
     "grade": false,
     "grade_id": "cell-2326178594a8f44c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "point_nn_clf = NeuralModule(5)\n",
    "point_nn_reg = NeuralModule(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.782631Z",
     "start_time": "2021-03-11T23:48:10.767672Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958fde08e4a9f04e633dc82bc85082dd",
     "grade": true,
     "grade_id": "cell-917f63ec6b575f59",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.813548Z",
     "start_time": "2021-03-11T23:48:10.783629Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bba77f886ccd469eb1b3c4370c830f",
     "grade": true,
     "grade_id": "cell-bd3bbcd6d22aa9b2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test the forward function\n",
    "n = 10\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_clf(inp)\n",
    "### BEGIN HIDDEN TEST\n",
    "n = 20\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_clf(inp)\n",
    "assert out.size(0) == n\n",
    "assert out.size(1) == 5\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.828509Z",
     "start_time": "2021-03-11T23:48:10.814546Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dfa481e9b325b14fdb26e39618d6169",
     "grade": true,
     "grade_id": "cell-1d92c755e64de89f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test the forward function\n",
    "n = 10\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_reg(inp)\n",
    "### BEGIN HIDDEN TEST\n",
    "n = 20\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_reg(inp)\n",
    "assert out.size(0) == n\n",
    "assert out.size(1) == 1\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Loss Functions (5 points)\n",
    "Pointwise LTR algorithms use pointwise loss functions.\n",
    "Usually, the popular loss functions for pointwise LTR are:\n",
    " - Cross entropy loss for classification (3 points)\n",
    " - Regression loss (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation (3 points):**\n",
    "Implement cross entropy loss and and then cross entropy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.837656Z",
     "start_time": "2021-03-11T23:48:10.829506Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a82ac14d42f8800a3fcf1fe5153dd1d3",
     "grade": false,
     "grade_id": "cell-d095f3c75bd11bc3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def clf_loss(output, target):\n",
    "    \"\"\"\n",
    "    Cross entropy loss - returns a single number. \n",
    "    output: (float) tensor, shape - [N, 5] \n",
    "    target: (float/long) tensor, shape - [N]. \n",
    "    \n",
    "    Hint: This function should also handle cases when target is either long/float types \n",
    "    \"\"\"\n",
    "    assert output.size(0) == target.size(0)\n",
    "    assert output.size(1) == 5\n",
    "    # YOUR CODE HERE\n",
    "    #loss_sample = np.einsum('ij,ij->i', target, np.log(output))\n",
    "    #out = -loss_sample.mean()\n",
    "    target = target.long()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    out = loss(output, target)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.869134Z",
     "start_time": "2021-03-11T23:48:10.837656Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec3127e3b21ab74c2771110450b46559",
     "grade": true,
     "grade_id": "cell-eb43efcf784d82d9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results: [1.5910069942474365, 1.600582480430603, 1.6723783016204834, 1.8065980672836304, 1.6385562419891357]\n",
      "expected results: [1.5910069942474365, 1.600582480430603, 1.6723783016204834, 1.8065979480743408, 1.6385562419891357]\n"
     ]
    }
   ],
   "source": [
    "## Test clf_loss\n",
    "g = torch.manual_seed(42)\n",
    "tests = [torch.rand(5, 5, generator=g) for _ in range(5)]\n",
    "target = torch.LongTensor([1, 2, 3, 4, 0])\n",
    "\n",
    "results = [1.5910069942474365, \n",
    "           1.600582480430603, \n",
    "           1.6723783016204834, \n",
    "           1.8065979480743408, \n",
    "           1.6385562419891357]\n",
    "\n",
    "l1 = [clf_loss(output, target).item() for output in tests]\n",
    "print(f'your results: {l1}')\n",
    "print(f'expected results: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.891815Z",
     "start_time": "2021-03-11T23:48:10.869134Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1e1871d067d534d41585fe9d657244a",
     "grade": false,
     "grade_id": "cell-d01649f26022bf4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (1 points)\n",
    "def clf_pred(inp, net):\n",
    "    \"\"\"\n",
    "    The output of the classifier network produces a [Nx5] output corresponding to \n",
    "    the relevance labels (each row does *not* add to 1!)\n",
    "    This function should predict the most probable relevance from the relevance labels\n",
    "    \n",
    "    inp: The input [N, num_features]\n",
    "    net: the neural network, takes in [N, num_features] and outputs [N, 5]\n",
    "    \n",
    "    return: a [N, 1] (long) tensor, the relevance labels\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    features = net(inp)\n",
    "    out = features.max(dim=1)[1] \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.922982Z",
     "start_time": "2021-03-11T23:48:10.894060Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e01b7aa6f5cebb78e604f9fa5d8da29",
     "grade": true,
     "grade_id": "cell-1f5c809567bf7f02",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results:[3 3 2 1 2 4 4 1 3 1]\n",
      "expected:[3 3 2 1 2 4 4 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "g = torch.manual_seed(42)\n",
    "def clf_(inp):\n",
    "    return torch.rand(inp.size(0), 5, generator=g)\n",
    "\n",
    "inp = torch.rand(10, 5, generator=g)\n",
    "r = np.array([3, 3, 2, 1, 2, 4, 4, 1, 3, 1])\n",
    "p = clf_pred(inp, clf_).numpy()\n",
    "print(f'your results:{p}')\n",
    "print(f'expected:{r}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.938940Z",
     "start_time": "2021-03-11T23:48:10.924977Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cf6191b552e4daea3f0fd4a91e15fd4",
     "grade": false,
     "grade_id": "cell-a4f04b744ef63756",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# to construct a predictor for a particular network, see this example\n",
    "# (this will be required in the next cell)\n",
    "clf_pred_fn = partial(clf_pred, net=point_nn_clf)\n",
    "# the 'net' argument doesn't need to be provided anymore!\n",
    "clf_pred_fn(torch.rand(5, data.num_features)).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e14973fb674e1e9adb553605e4e7b333",
     "grade": false,
     "grade_id": "cell-d683efd6ca306e81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (2 points):**\n",
    "Implement regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.954316Z",
     "start_time": "2021-03-11T23:48:10.939937Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f2b905ddb927ed981bb294825674993",
     "grade": false,
     "grade_id": "cell-c024ed97d7100038",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def reg_loss(output, target):\n",
    "    \"\"\"\n",
    "    Regression loss - returns a single number. \n",
    "    Make sure to use the TODO loss!\n",
    "    output: (float) tensor, shape - [N, 1] \n",
    "    target: (float) tensor, shape - [N]. \n",
    "    \"\"\"\n",
    "    assert target.dim() == 1\n",
    "    assert output.size(0) == target.size(0)\n",
    "    assert output.size(1) == 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    loss = nn.MSELoss()\n",
    "    out = loss(output, target)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:10.977508Z",
     "start_time": "2021-03-11T23:48:10.954316Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "064dde728a201a97f97ca266c8641a0e",
     "grade": true,
     "grade_id": "cell-24edd9d567aac9da",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Test reg_loss\n",
    "g = torch.manual_seed(42)\n",
    "output = [torch.randint(low=0, high=5, size=(5, 1), generator=g).float() for _ in range(5)]\n",
    "target = torch.randint(low=0, high=5, size=(5,), generator=g).float()\n",
    "\n",
    "l = [reg_loss(o, target).item() for o in output]\n",
    "r = [4.800000190734863, \n",
    "     3.0, \n",
    "     7.599999904632568, \n",
    "     5.400000095367432, \n",
    "     0.6000000238418579]\n",
    "print(f'your results:{l}')\n",
    "print(f'expected:{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0eddb61dde344891ea7efc8fdd67752f",
     "grade": false,
     "grade_id": "cell-0977a61ec0cfa7ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (45 points):**\n",
    "Now implement a wrapper for training a pointwise LTR, that takes the model and the loss function as input and trains the model.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 5 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Appropriate loss function & prediction function: 5 points\n",
    " - Both classification / regression models handled appropriately: 5 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:11.004969Z",
     "start_time": "2021-03-11T23:48:10.982327Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbfd2a686ecc7ab254a4e5e9b332119",
     "grade": false,
     "grade_id": "cell-9361533c572e304b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (45 points)\n",
    "def train_pointwise(net, loss, params):\n",
    "    \"\"\"\n",
    "    This function should train a Pointwise network, \n",
    "    trained based on the loss (either \"clf\" / \"reg\"). \n",
    "    \n",
    "    The network is trained using the Adam optimizer\n",
    "        \n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    \n",
    "    \n",
    "    Hints:\n",
    "    1. Use the LTRData class defined above\n",
    "    2. You will have to construct a partial function if loss=\"clf\" \n",
    "       before using it in evaluate_model() (see cells after the defn of clf_pred)\n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    loss: one of \"clf\" or \"reg\"\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert loss in {\"clf\", \"reg\"}\n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    train_dl = DataLoader(LTRData(data, \"train\"), batch_size=params.batch_size, shuffle=True)\n",
    "    optimiser = Adam(net.parameters(), lr=params.lr)\n",
    "    net.train()\n",
    "    for i in range(params.epochs):\n",
    "        for (x, y) in train_dl:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = net(x)\n",
    "            if loss == 'clf':\n",
    "                loss_value = clf_loss(outputs, y)\n",
    "            elif loss == 'reg':\n",
    "                loss_value = reg_loss(outputs, y)\n",
    "            loss_value.backward()\n",
    "            optimiser.step()\n",
    "        if loss == 'clf':\n",
    "            clf_pred_fn = partial(clf_pred, net=net)         \n",
    "            dict_epoch_train = {}\n",
    "            dict_values_train = evaluate_model(clf_pred_fn, \"train\", print_results=False)\n",
    "            select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "            dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "            train_metrics_epoch.append(dict_epoch_train)\n",
    "            dict_epoch_eval = {}\n",
    "            dict_values_eval = evaluate_model(clf_pred_fn, \"validation\", print_results=False)\n",
    "            select_m_eval = {k: v for (k, v) in dict_values_eval.items() if k in params.metrics}\n",
    "            dict_epoch_eval[f'Epoch {i+1}'] = select_m_eval\n",
    "            val_metrics_epoch.append(dict_epoch_eval) \n",
    "        elif loss == 'reg':\n",
    "            dict_epoch_train = {}\n",
    "            dict_values_train = evaluate_model(net, \"train\", print_results=False)\n",
    "            select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "            dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "            train_metrics_epoch.append(dict_epoch_train)\n",
    "            dict_epoch_eval = {}\n",
    "            dict_values_eval = evaluate_model(net, \"validation\", print_results=False)\n",
    "            select_m_eval = {k: v for (k, v) in dict_values_eval.items() if k in params.metrics}\n",
    "            dict_epoch_eval[f'Epoch {i+1}'] = select_m_eval\n",
    "            val_metrics_epoch.append(dict_epoch_eval)\n",
    "                  \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:11.020590Z",
     "start_time": "2021-03-11T23:48:11.004969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change this to test your code!\n",
    "pointwise_test_params = Namespace(epochs=2, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                   metrics={\"ndcg\"})\n",
    "# uncomment to test your code\n",
    "##train a regression model\n",
    "#met_reg = train_pointwise(point_nn_reg, \"reg\", pointwise_test_params)\n",
    "## train a classification model\n",
    "#met_clf = train_pointwise(point_nn_clf, \"clf\", pointwise_test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "738eaae8abdea0f21d248a0d677bc424",
     "grade": false,
     "grade_id": "cell-27ec0e0dd8a5d98d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell is used to generate reproducible results which should be submitted with the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:48:11.028190Z",
     "start_time": "2021-03-11T23:48:11.020590Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a485ad0b3f14de6ee384fa6b42ab36f",
     "grade": false,
     "grade_id": "cell-11e8cbc591a51256",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def create_results(net, train_fn, prediction_fn, results_file, *train_params):\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    metrics = train_fn(net, *train_params)\n",
    "    net.eval()\n",
    "    test_metrics, test_qq = evaluate_model(prediction_fn, \"test\", print_results=True, q_level=True)\n",
    "    \n",
    "    \n",
    "    test_q = {}\n",
    "    for m in {\"ndcg\", \"precision@05\", \"recall@05\"}:\n",
    "        test_q[m] = test_qq[m]\n",
    "    \n",
    "    with open(results_file, \"w\") as writer:\n",
    "        json.dump({\n",
    "            \"metrics\": metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"test_query_level_metrics\": test_q,\n",
    "        }, writer, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5926b542770cebafc36574bbff9b7d3e",
     "grade": false,
     "grade_id": "cell-16ed543545863f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now use the above functions to generate your `json` files for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:49:00.902236Z",
     "start_time": "2021-03-11T23:48:11.028190Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eef82389f05e60a59c9bddefc6570264",
     "grade": false,
     "grade_id": "cell-cb8314e4e579adac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "params_regr = Namespace(epochs=11, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "pointwise_regression_model = NeuralModule(1)\n",
    "create_results(pointwise_regression_model, \n",
    "               train_pointwise, \n",
    "               pointwise_regression_model,\n",
    "               \"./pointwise_regression.json\", \n",
    "               \"reg\", params_regr)\n",
    "# persist models\n",
    "torch.save(pointwise_regression_model.state_dict(), \"./pointwise_regr_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.232281Z",
     "start_time": "2021-03-11T23:49:00.902236Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ddac4c7614df42d2655e2304eba1c21",
     "grade": false,
     "grade_id": "cell-8b25e13a53ad95ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "params_clf = Namespace(epochs=13, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "pointwise_classification_model = NeuralModule(5)\n",
    "create_results(pointwise_classification_model,\n",
    "               train_pointwise,\n",
    "               partial(clf_pred, net=pointwise_classification_model),\n",
    "               \"./pointwise_classification.json\", \"clf\", params_clf)\n",
    "\n",
    "torch.save(pointwise_classification_model.state_dict(), \"./pointwise_clf_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ab42dfe7a4ffdce839f7f3990147830",
     "grade": true,
     "grade_id": "cell-ccb9cdbf2280bcff",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.254829Z",
     "start_time": "2021-03-11T23:50:08.234276Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8123d91619096913bd0809c27781857",
     "grade": true,
     "grade_id": "cell-780585f47729739e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./pointwise_regression.json\")\n",
    "assert os.path.exists(\"./pointwise_classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8a82c097df5799c15c9000475ad11e",
     "grade": false,
     "grade_id": "cell-e48bb26c37eacea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3: Pairwise LTR (60 points) <a class=\"anchor\" id=\"pairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section,  you will learn and implement RankNet, a  pairwise learning to rank algorithm.\n",
    "\n",
    "For a given query, consider two documents $D_i$ and $D_j$ with two different ground truth relevance  labels,  with  feature  vectors $x_i$ and $x_j$ respectively.   The  RankNet  model,  just  like  the pointwise model, uses $f$ to predict scores i.e $s_i=f(x_i)$ and $s_j=f(x_j)$, but uses a different loss during  training. $D_i \\triangleright D_j$ denotes  the  event  that $D_i$ should  be  ranked  higher  than $D_j$.   The  two outputs $s_i$ and $s_j$ are mapped to a learned probability that $D_i \\triangleright D_j$: \n",
    "\n",
    "\n",
    "$$        P_{ij} = \\frac{1}{1 + e^{-\\sigma(s_i - s_j)}} $$\n",
    "  \n",
    "where $\\sigma$ is a parameter that determines the shape of the sigmoid. The loss of the RankNet model is the cross entropy cost function:\n",
    "\n",
    "$$        C = - \\bar{P}_{ij} \\log P_{ij} - (1-\\bar{P}_{ij}) \\log (1 - P_{ij}) $$\n",
    "\n",
    "As the name suggests, in the pairwise approach to LTR, we optimize a loss $l$ over pairs of documents. Let $S_{ij} \\in \\{0, \\pm1 \\}$ be equal to $1$ if the relevance of document $i$ is greater than document $j$; $-1$ if document $j$ is more relevant than document $i$; and 0 if they have the same relevance. This gives us $\\bar{P}_{ij} = \\frac{1}{2} (1 + S_{ij})$ so that $\\bar{P}_{ij} = 1$ if $D_i \\triangleright D_j$; $\\bar{P}_{ij} = 0$ if $D_j \\triangleright D_i$; and finally $\\bar{P}_{ij} = \\frac{1}{2}$ if the relevance is identical. This gives us:\n",
    "\n",
    "$$        C = \\frac{1}{2}(1- S_{ij})\\sigma(s_i - s_j) + \\log(1+ e^{-\\sigma(s_i - s_j)}) $$\n",
    "\n",
    "Now, consider a single query for which $n$ documents have been returned. Let the output scores of the ranker be $s_j$ ; $j=\\{1, \\dots, n \\}$, the model parameters be $w_k \\in \\mathbb{R}^W$, and let the set of pairs of document indices used for training be $\\mathcal{P}$. Then, the total cost is $C_T = \\sum_{i,j \\in \\mathcal{P}} C(s_i; s_j)$. \n",
    "\n",
    "\n",
    "\n",
    "- Implement RankNet. You should construct training samples by creating all possible pairs of documents for a given query and optimizing the loss above. Use the following parameters:\n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow 1$, where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLu`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e80a1fc2830a7bfe3be62c3bbf1df5b7",
     "grade": false,
     "grade_id": "cell-5359ecd282448c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the pairwise loss, we need to have a structured **dataloader** which detects the documents associated with a specific query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.301680Z",
     "start_time": "2021-03-11T23:50:08.257799Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e50a3b3ef0bf4fba2f792073ebb8443",
     "grade": false,
     "grade_id": "cell-0009b5254fc5f2ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QueryGroupedLTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        self.split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert self.split is not None, \"Invalid split!\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.split.num_queries()\n",
    "\n",
    "    def __getitem__(self, q_i):\n",
    "        feature = torch.FloatTensor(self.split.query_feat(q_i))\n",
    "        labels = torch.FloatTensor(self.split.query_labels(q_i))\n",
    "        return q_i, feature, labels\n",
    "\n",
    "# the return types are different from what pytorch expects, \n",
    "# so we will define a custom collate function which takes in\n",
    "# a batch and returns tensors (qids, features, labels) \n",
    "def qg_collate_fn(batch):\n",
    "    \n",
    "    qids = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for (q, f, l) in batch:\n",
    "        qids.append(q)\n",
    "        features.append(f)\n",
    "        labels.append(l)\n",
    "    \n",
    "    return qids, features, labels\n",
    "    \n",
    "    \n",
    "## example - NOTE the collate_fn argument!\n",
    "train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), batch_size=1, shuffle=True, collate_fn=qg_collate_fn)\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "for (qids, x, y) in train_dl:\n",
    "    # different from the previous data loader, qids, x and y aren't tensors, but lists!\n",
    "    for q_i, features_i, labels_i in zip(qids, x, y):\n",
    "        print(f\"Query {q_i} has {len(features_i)} query-document pairs\")\n",
    "        print(f\"Shape of features for Query {q_i}: {features_i.size()}\")\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c719c1aca6aa05893f903f78d75ba94",
     "grade": false,
     "grade_id": "cell-acdb1bfcd2ec582e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (25 points):**\n",
    "First, implement the pairwaise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Proper application of the formula: 10 points\n",
    " - Mean loss: 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.326523Z",
     "start_time": "2021-03-11T23:50:08.303675Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dcefb2b21b4524aa03cdf22382934ba",
     "grade": false,
     "grade_id": "cell-3a612aeb9e982639",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (25 points)\n",
    "def pairwise_loss(scores, labels):\n",
    "    \"\"\"\n",
    "    Compute and return the pairwise loss *for a single query*. To compute this, compute the loss for each \n",
    "    ordering in a query, and then return the mean. Use sigma=1.\n",
    "    \n",
    "    For a query, consider all possible ways of comparing 2 document-query pairs.\n",
    "    \n",
    "    Hint: See the next cell for an example which should make it clear how the inputs look like\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    \"\"\"\n",
    "    # if there's only one rating\n",
    "    if labels.size(0) < 2:\n",
    "        return None\n",
    "    # YOUR CODE HERE\n",
    "    n = scores.size(0)\n",
    "    if scores.size() != (n,1):\n",
    "        scores = torch.unsqueeze(scores, 1)\n",
    "    distances = scores - torch.transpose(scores, 0, 1)\n",
    "    expand_labels = torch.unsqueeze(labels, 1)\n",
    "    diff_labels = expand_labels - torch.transpose(expand_labels, 0, 1)\n",
    "    pos = (diff_labels > 0) * torch.ones(n)\n",
    "    neg = (diff_labels < 0) * (-torch.ones(n))\n",
    "    S_ij = pos + neg\n",
    "    loss_per_pair = 0.5 * (1 - S_ij) * distances + torch.log(1 + torch.exp(-distances))\n",
    "    loss_per_pair = loss_per_pair.masked_select(~torch.eye(n, dtype=bool)).view(n, n - 1)\n",
    "    final_loss = torch.mean(loss_per_pair)\n",
    "    return final_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.364531Z",
     "start_time": "2021-03-11T23:50:08.326523Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "234a4669f7c7e14949006be676cabb90",
     "grade": false,
     "grade_id": "cell-01f6e909bc892bc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's say we have 2 queries, the first one with 5 <document, query> pairs \n",
    "#    and the second one with 2 <document, query> pairs. The two variables can\n",
    "#    look something like this (note the shape, not the values):\n",
    "\n",
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0])\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7])\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "print(pairwise_loss(scores_1, labels_1))\n",
    "print(pairwise_loss(scores_2, labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.394166Z",
     "start_time": "2021-03-11T23:50:08.366526Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "618bd72120cdd1baae3f22733f124d6c",
     "grade": true,
     "grade_id": "cell-5f706c38e99721df",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b63a41669c7a768f5420d43968a53212",
     "grade": false,
     "grade_id": "cell-45f14561e4843320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (35 points):**\n",
    "Now implement the wrapper for the pairwise LTR.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset\n",
    " - and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.409787Z",
     "start_time": "2021-03-11T23:50:08.394166Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ad65b3cbd923af54ff56f804bbc93c",
     "grade": false,
     "grade_id": "cell-a85c38ca94031203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (35 points)\n",
    "def train_pairwise(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the pairwise loss\n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    Hint: Consider the case when the loss function returns 'None'\n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \"\"\"\n",
    "\n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), batch_size=1, shuffle=True, collate_fn=qg_collate_fn)\n",
    "    optimiser = Adam(net.parameters(), lr=params.lr)\n",
    "    net.train()\n",
    "    for i in range(params.epochs):\n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):\n",
    "                optimiser.zero_grad()\n",
    "                outputs = net(features_i)\n",
    "                loss_value = pairwise_loss(outputs, labels_i)\n",
    "                loss_value.backward()\n",
    "                optimiser.step()\n",
    "                    \n",
    "        dict_epoch_train = {}\n",
    "        dict_values_train = evaluate_model(net, \"train\", print_results=False)\n",
    "        select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "        dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "        train_metrics_epoch.append(dict_epoch_train)\n",
    "        dict_epoch_eval = {}\n",
    "        dict_values_eval = evaluate_model(net, \"validation\", print_results=False)\n",
    "        select_m_eval = {k: v for (k, v) in dict_values_eval.items() if k in params.metrics}\n",
    "        dict_epoch_eval[f'Epoch {i+1}'] = select_m_eval\n",
    "        val_metrics_epoch.append(dict_epoch_eval) \n",
    "                                        \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.425409Z",
     "start_time": "2021-03-11T23:50:08.409787Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4b97202b1befddb4af2ec7851c5d174",
     "grade": false,
     "grade_id": "cell-19ec0cf692c86b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pairwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "## uncomment to test your code\n",
    "# pairwise_net = NeuralModule(1)\n",
    "# train_pairwise(pairwise_net, pairwise_params_test)\n",
    "# pairwise_test, pairwise_q = evaluate_model(pairwise_net,\n",
    "#                                          \"test\", print_results=True, q_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.447847Z",
     "start_time": "2021-03-11T23:50:08.425409Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52558867a0a7f79d6c42ceab4f26a132",
     "grade": true,
     "grade_id": "cell-34178113ea5e9331",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3867bfe2e108bffb3ae69f5ddfd68834",
     "grade": false,
     "grade_id": "cell-3a95bb01f72fc76c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 4: Pairwise: Speed-up RankNet (70 points) <a class=\"anchor\" id=\"SpairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "To speed up training of the previous model, we can consider a sped up version of the model, where instead of `.backward` on the loss, we use `torch.backward(lambda_i)`. \n",
    "\n",
    "The derivative of the total cost $C_T$ with respect to the model parameters $w_k$ is:\n",
    "\n",
    "$$        \\frac{\\partial C_T}{\\partial w_k} = \\sum_{(i,j) \\in \\mathcal{P}} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial C(s_i, s_j)}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k} $$\n",
    "\n",
    "We can rewrite this sum by considering the set of indices $j$ , for which $\\{i,j\\}$ is a valid pair, denoted by $\\mathcal{P}_i$, and the set of document indices $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\sum_{j \\in \\mathcal{P}_i} \n",
    "\\frac{\\partial C(s_i, s_j)}{\\partial s_i} \n",
    "$$\n",
    "\n",
    "This sped of version of the algorithm first computes scores $s_i$ for all the documents. Then for each $j= 1, \\dots, n$, compute:\n",
    "\n",
    "$$\n",
    "\\lambda_{ij} = \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sigma \\bigg( \\frac{1}{2}(1 - S_{ij}) -  \\frac{1}{1 + e^{\\sigma(s_i -s_j))}} \\bigg) \\\\\n",
    "\\lambda_i = \\sum_{j \\in \\mathcal{P}_i} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sum_{j \\in \\mathcal{P}_i} \\lambda_{ij}\n",
    "$$\n",
    "\n",
    "That gives us:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\lambda_i\n",
    "$$\n",
    "\n",
    "This can be directly optimized in pytorch using: `torch.autograd.backward(scores, lambda_i)` \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c88f76d295dd1dc2778cd2413b4c58e8",
     "grade": false,
     "grade_id": "cell-2a9b7b682a011642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (20 points):**\n",
    "Implement the sped-up version of pairwise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Proper application of the formula: 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.462789Z",
     "start_time": "2021-03-11T23:50:08.447847Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cea38cdd68cb70d08e33827bffc53a8",
     "grade": false,
     "grade_id": "cell-ba7f8d8631e3f1d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def compute_lambda_i(scores, labels):\n",
    "    \"\"\"\n",
    "    Compute \\lambda_i (defined in the previous cell). (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    return: \\lambda_i, a tensor of shape: [N, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    n = scores.size(0)\n",
    "    if scores.size() != (n,1):\n",
    "        scores = torch.unsqueeze(scores, 1)\n",
    "    distances = scores - torch.transpose(scores, 0, 1)\n",
    "    expand_labels = torch.unsqueeze(labels, 1)\n",
    "    diff_labels = expand_labels - torch.transpose(expand_labels, 0, 1)\n",
    "    pos = (diff_labels > 0) * torch.ones(n)\n",
    "    neg = (diff_labels < 0) * (-torch.ones(n))\n",
    "    S_ij = pos + neg\n",
    "    lambda_i = 0.5 * (1 - S_ij) - (1 / (1 + torch.exp(distances)))\n",
    "    lambda_i = torch.sum(lambda_i, dim=1, keepdim=True)\n",
    "    return lambda_i\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.506382Z",
     "start_time": "2021-03-11T23:50:08.465780Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c4533f39d3dfbefdd4b56c61316eac8",
     "grade": true,
     "grade_id": "cell-756179237da34c57",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0])\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7])\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "print(compute_lambda_i(scores_1, labels_1))\n",
    "print(compute_lambda_i(scores_2, labels_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00482b2615dc28ead4c745a705e5fafb",
     "grade": false,
     "grade_id": "cell-ed55c62fbba08923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (50 points):**\n",
    "Using the sped-up loss function `compute_lambda_i`, implement the spedup wrapper for pairwise training.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.530159Z",
     "start_time": "2021-03-11T23:50:08.506382Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14e4da9878b2e0a3603437b29a66a07c",
     "grade": false,
     "grade_id": "cell-ddfeb927c2f4a31c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (50 points)\n",
    "def train_pairwise_spedup(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the sped up pairwise loss\n",
    "    \n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \"\"\"\n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "        \n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), batch_size=1, shuffle=True, collate_fn=qg_collate_fn)\n",
    "    optimiser = Adam(net.parameters(), lr=params.lr)\n",
    "    net.train()\n",
    "    for i in range(params.epochs):\n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):\n",
    "                optimiser.zero_grad()\n",
    "                outputs = net(features_i)\n",
    "                lambda_i = compute_lambda_i(outputs, labels_i)\n",
    "                torch.autograd.backward(outputs, lambda_i)\n",
    "                optimiser.step()\n",
    "        \n",
    "        dict_epoch_train = {}\n",
    "        dict_values_train = evaluate_model(net, \"train\", print_results=False)\n",
    "        select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "        dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "        train_metrics_epoch.append(dict_epoch_train)\n",
    "        dict_epoch_eval = {}\n",
    "        dict_values_eval = evaluate_model(net, \"validation\", print_results=False)\n",
    "        select_m_eval = {k: v for (k, v) in dict_values_eval.items() if k in params.metrics}\n",
    "        dict_epoch_eval[f'Epoch {i+1}'] = select_m_eval\n",
    "        val_metrics_epoch.append(dict_epoch_eval)  \n",
    "    \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:50:08.545781Z",
     "start_time": "2021-03-11T23:50:08.530159Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb9b27b6ec712d8d1146751bc150791e",
     "grade": false,
     "grade_id": "cell-f80dc2eae24968ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code\n",
    "# pairwise_spedup_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg@10\",\"ndcg\"})\n",
    "# pairwise_net_spedup = NeuralModule(1)\n",
    "# train_pairwise_spedup(pairwise_net_spedup, pairwise_spedup_params_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c173b7020a1122f7218bbf37ee370884",
     "grade": false,
     "grade_id": "cell-20aa326063f673c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell creates the results file you will have to submit. *Note that the next cell trains and evaluates only the sped-up version - this is intentional!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.140393Z",
     "start_time": "2021-03-11T23:50:08.545781Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77211db554a4bb37582d7c61c7170243",
     "grade": false,
     "grade_id": "cell-32660e207ce6d16a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "params = Namespace(epochs=8, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "pairwise_model = NeuralModule(1)\n",
    "\n",
    "create_results(pairwise_model, \n",
    "               train_pairwise_spedup, \n",
    "               pairwise_model,\n",
    "               \"./pairwise.json\",\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(pairwise_model.state_dict(), \"./pairwise_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cc6f1a702be838b6e26733b089d5bda",
     "grade": true,
     "grade_id": "cell-8aad7ba1f8c6c23c",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.156352Z",
     "start_time": "2021-03-11T23:51:54.142388Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b46d1ebc1676bc1dbce629b0a978326",
     "grade": true,
     "grade_id": "cell-3dc8ee835753e5aa",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./pairwise.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60ad8126d68fcf4814988ad22c335c2a",
     "grade": false,
     "grade_id": "cell-14e048f55b2e6aea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Section 5: Listwise LTR (80 points) <a class=\"anchor\" id=\"listwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section, you will implement LambdaRank, a listwise approach to LTR. Consider the computation of $\\lambda$ for sped-up RankNet (that you've already implemented). $\\lambda$ here amounts to the 'force' on a document given its neighbours in the ranked list. The design of $\\lambda$ in LambdaRank is similar to RankNet, but is scaled by DCG gain from swapping the two documents in question. Let's suppose that the corresponding ranks of doucment $D_i$ and $D_j$ are $r_i$ and $r_j$ respectively. Given a ranking measure $IRM$, such as $NDCG$ or $ERR$, the lambda function in LambdaRank is defined as:\n",
    "\n",
    "\n",
    "$$        \\frac{\\partial C}{\\partial s_i} = \\sum_{j \\in D} \\lambda_{ij} \\cdot |\\bigtriangleup IRM (i,j)| $$\n",
    "\n",
    "Where $|\\bigtriangleup IRM(i,j)|$ is the absolute difference in $IRM$ after swapping the rank positions $r_i$ and $r_j$ while leaving everything else unchanged ($| \\cdot |$ denotes the absolute value). Note that we do not backpropogate $|\\bigtriangleup IRM|$, it is treated as a constant that scales the gradients. In this assignment we will use $|\\bigtriangleup NDCG|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0caedecf5dcfd8b561a99f58ea756abd",
     "grade": false,
     "grade_id": "cell-351c194e6797d0a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (30 points):**\n",
    "Implement the listwise loss.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Computing $|\\bigtriangleup NDCG|$: 10 points \n",
    " - Proper application of the formula: 10 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.172310Z",
     "start_time": "2021-03-11T23:51:54.157348Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0ba4266e951c80236d3dc6f0e0c32a5",
     "grade": false,
     "grade_id": "cell-48f6a2a1c4a529b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (30 points)\n",
    "def listwise_loss(scores, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the LambdaRank loss. (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    returns: a tensor of size [N, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    n = scores.size(0)\n",
    "    if scores.size() != (n,1):\n",
    "        scores = torch.unsqueeze(scores, 1)\n",
    "\n",
    "    random_i = np.random.permutation(np.arange(np.asarray(scores.detach()).shape[0]))\n",
    "    labels_np = np.asarray(labels.detach())[random_i]\n",
    "    scores_np = np.asarray(scores.flatten().detach())[random_i]\n",
    "    sort_ind = np.argsort(scores_np)[::-1]\n",
    "    sorted_labels = labels_np[sort_ind]\n",
    "    ideal_labels = np.sort(labels_np)[::-1]\n",
    "    epsilon = 0.00000000001 \n",
    "    idcg = evaluate.dcg_at_k(ideal_labels, 0)\n",
    "    idcg = 1.0 / (idcg + epsilon) #We add epsilon so that when idcg = 0 (when all labels are zero) we don't divide by zero\n",
    "    \n",
    "    k = sorted_labels.shape[0]\n",
    "    order_rank = torch.arange(k).view(-1, 1)\n",
    "    sorted_labels2 = torch.unsqueeze(torch.from_numpy(sorted_labels), 1)\n",
    "    nom_diff = torch.pow(2.0, sorted_labels2) - torch.pow(2.0, torch.transpose(sorted_labels2, 0, 1))\n",
    "    denom_diff = (1.0 / torch.log2(order_rank + 2.0)) - (1.0 / torch.log2(order_rank.t() + 2.0))\n",
    "    delta_ndcg = torch.abs(idcg * nom_diff * denom_diff)\n",
    "\n",
    "    distances = scores - torch.transpose(scores, 0, 1)\n",
    "    expand_labels = torch.unsqueeze(labels, 1)\n",
    "    diff_labels = expand_labels - torch.transpose(expand_labels, 0, 1)\n",
    "    pos = (diff_labels > 0) * torch.ones(n)\n",
    "    neg = (diff_labels < 0) * (-torch.ones(n))\n",
    "    S_ij = pos + neg\n",
    "    lambda_i = 0.5 * (1 - S_ij) - (1 / (1 + torch.exp(distances)))\n",
    "\n",
    "    lambda_r = lambda_i * delta_ndcg\n",
    "    lambda_r = torch.sum(lambda_r, dim=1, keepdim=True)\n",
    "    return lambda_r    \n",
    "    \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.204222Z",
     "start_time": "2021-03-11T23:51:54.174302Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28f708cd49db29860a2774a320685d18",
     "grade": false,
     "grade_id": "cell-7ffca014e68d2c32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0])\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7])\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "\n",
    "print(listwise_loss(scores_1, labels_1))\n",
    "\n",
    "\n",
    "\n",
    "print(listwise_loss(scores_2, labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.220180Z",
     "start_time": "2021-03-11T23:51:54.206217Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3c154e9791120fcb899928865f40d30",
     "grade": true,
     "grade_id": "cell-fadab94bb19ea7ee",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48df2b36472c6f03f84a293fea5d3602",
     "grade": false,
     "grade_id": "cell-7dec2c279f77b272",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### **Implementation (50 points):**\n",
    "And use the loss function above to train a listwise LTR.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.243487Z",
     "start_time": "2021-03-11T23:51:54.222174Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "786e51ea263d0ea82a5ec251344f7d0e",
     "grade": false,
     "grade_id": "cell-34c4d0bdbb753580",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (50 points)\n",
    "def train_listwise(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the listwise (LambdaRank) loss\n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "        \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \"\"\"\n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), batch_size=1, shuffle=True, collate_fn=qg_collate_fn)\n",
    "    optimiser = Adam(net.parameters(), lr=params.lr)\n",
    "    net.train()\n",
    "    for i in range(params.epochs):\n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):\n",
    "                optimiser.zero_grad()\n",
    "                outputs = net(features_i)\n",
    "                loss = listwise_loss(outputs, labels_i)\n",
    "                torch.autograd.backward(outputs, loss)\n",
    "                optimiser.step()\n",
    "                    \n",
    "        dict_epoch_train = {}\n",
    "        dict_values_train = evaluate_model(net, \"train\", print_results=False)\n",
    "        select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "        dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "        train_metrics_epoch.append(dict_epoch_train)\n",
    "        dict_epoch_eval = {}\n",
    "        dict_values_eval = evaluate_model(net, \"validation\", print_results=False)\n",
    "        select_m_eval = {k: v for (k, v) in dict_values_eval.items() if k in params.metrics}\n",
    "        dict_epoch_eval[f'Epoch {i+1}'] = select_m_eval\n",
    "        val_metrics_epoch.append(dict_epoch_eval)     \n",
    "    \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:51:54.259328Z",
     "start_time": "2021-03-11T23:51:54.243487Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "109d4be0105ee65f2e446c5d1a961eea",
     "grade": false,
     "grade_id": "cell-36910488e505a73a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code\n",
    "# listwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "# listwise_net = NeuralModule(1)\n",
    "# train_listwise(listwise_net, listwise_params_test)\n",
    "# evaluate_model(listwise_net, \"test\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "275a63cf01f8efa9cb9b42b45d41aa9b",
     "grade": false,
     "grade_id": "cell-27de808a4c9d6bf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following cell saves your results in `listwise.json` file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:53:22.045432Z",
     "start_time": "2021-03-11T23:51:54.260321Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b50d890fdac41f183467a165cc71ae6",
     "grade": false,
     "grade_id": "cell-8c3dde9e1dcde277",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "params = Namespace(epochs=8, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "listwise_model = NeuralModule(1)\n",
    "\n",
    "create_results(listwise_model, \n",
    "               train_listwise, \n",
    "               listwise_model,\n",
    "               \"./listwise.json\",\n",
    "               params)\n",
    "\n",
    "# persist model\n",
    "torch.save(listwise_model.state_dict(), \"./listwise_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbccf7ada9159548390a5d8d28df92d1",
     "grade": true,
     "grade_id": "cell-6c37b60a428d000a",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T23:53:22.060512Z",
     "start_time": "2021-03-11T23:53:22.045432Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e086691009a4dc0f8013c9f7f2d61bf",
     "grade": true,
     "grade_id": "cell-6c8d3b2e4ab668b1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./listwise.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a1aaed7333acfcbdf24046a15cc5a2b",
     "grade": false,
     "grade_id": "cell-e47b21d69c9be1e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 6: Comparing Pointwise, Pairwise and Listwise (70 points) <a class=\"anchor\" id=\"evaluation1\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In the next few cells, we will compare the methods you've implemented. Helper functions are provided for you, which you can use to make some conclusions. You can modify the code as needed (you are encouraged to do so!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T01:10:37.120450Z",
     "start_time": "2021-03-12T01:10:37.017725Z"
    }
   },
   "outputs": [],
   "source": [
    "def autolabel(ax, rects, labels):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for l, rect in zip(labels, rects):\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(l),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                   fontsize=7)\n",
    "\n",
    "def compare_methods(labels, metrics, metrics_to_plot={\"ndcg\", \"precision@05\", \"recall@05\"}):\n",
    "    \"\"\"\n",
    "    Constructs bar plots to compare different methods. \n",
    "    \n",
    "    labels: list/tuple of length N\n",
    "    metrics: list/tuple of length N, containing dictionary containing the test set results \n",
    "    metrics_to_plot: set of metrics to plot - each metric creates a separate plot \n",
    "    \"\"\"\n",
    "    assert len(metrics) == len(labels)\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot)) \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(metrics_to_plot), sharey=False)\n",
    "    fig.set_figheight(7)\n",
    "    fig.set_figwidth(15)\n",
    "    \n",
    "    colors = cm.get_cmap(\"Set1\").colors\n",
    "\n",
    "    for metric, ax, c in zip(metrics_to_plot, axes, colors):\n",
    "        m = [_[metric][0] for _ in metrics]  \n",
    "        std = [_[metric][1] for _ in metrics]\n",
    "        x = np.arange(len(labels))\n",
    "        rects = ax.bar(x, m, label=metric, color=c)\n",
    "        \n",
    "        l = [\"{0:.4f}({1:.4f})\".format(_[metric][0], _[metric][1]) for _ in metrics]\n",
    "        autolabel(ax, rects, l)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "        ax.set_yticks(np.linspace(0, 1, num=11))\n",
    "        ax.set_ylim(ymin=min(m) - 0.05, ymax= max(m) + 0.05)\n",
    "        ax.set_title(metric)\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_distribution(labels, q_metrics, metric=\"ndcg\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of NDCG scores\n",
    "    \n",
    "    labels: list/tuple of length N\n",
    "    q_metrics: list/tuple of dictionaries with length N, containing the query level results\n",
    "    metric: the metric to plot\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(labels)\n",
    "    # nC2\n",
    "    n_plots = int((n*(n-1))/2)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=n_plots, ncols=1)\n",
    "    fig.set_figheight(8 * n_plots)\n",
    "    fig.set_figwidth(10)\n",
    "    \n",
    "    colors = cm.get_cmap(\"Set1\").colors\n",
    "    \n",
    "    for idx, (i, j) in enumerate(itertools.combinations(range(n), 2)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        im = q_metrics[i][metric]\n",
    "        jm = q_metrics[j][metric]\n",
    "        \n",
    "        ax.hist(im, bins=50, label=labels[i], color=colors[i], alpha=0.55)\n",
    "        ax.hist(jm, bins=50, label=labels[j], color=colors[j], alpha=0.55)\n",
    "        \n",
    "        ax.set_title(f\"{labels[i]} vs {labels[j]}\")\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_xlabel(\"NDCG (binned)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T00:48:41.565839Z",
     "start_time": "2021-03-12T00:48:41.541901Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10071c1830b37c08ea6accb26accb372",
     "grade": false,
     "grade_id": "cell-b8d4e6f2d2e78550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load metrics and models\n",
    "\n",
    "pointwise_regr_model = NeuralModule(1)\n",
    "pointwise_regr_model.load_state_dict(torch.load(\"./pointwise_regr_wt\"))\n",
    "\n",
    "pointwise_clf_model = NeuralModule(5)\n",
    "pointwise_clf_model.load_state_dict(torch.load(\"./pointwise_clf_wt\"))\n",
    "pointwise_clf_pred_fn = partial(clf_pred, net=pointwise_clf_model)\n",
    "\n",
    "pairwise_model = NeuralModule(1)\n",
    "pairwise_model.load_state_dict(torch.load(\"./pairwise_wt\"))\n",
    "\n",
    "listwise_model = NeuralModule(1)\n",
    "listwise_model.load_state_dict(torch.load(\"./listwise_wt\"))\n",
    "\n",
    "\n",
    "methods = [\n",
    "    {\"results_file\": \"./pointwise_regression.json\", \"label\": \"Pointwise (R)\"},\n",
    "    {\"results_file\": \"./pointwise_classification.json\", \"label\": \"Pointwise (C)\"},\n",
    "    {\"results_file\": \"./pairwise.json\", \"label\": \"Pairwise\"},\n",
    "    {\"results_file\": \"./listwise.json\", \"label\": \"Listwise\"}\n",
    "]\n",
    "\n",
    "labels = []\n",
    "results = []\n",
    "q_results = []\n",
    "for m in methods:\n",
    "    labels.append(m[\"label\"])\n",
    "    \n",
    "    with open(m[\"results_file\"]) as reader:\n",
    "        r = json.load(reader)\n",
    "    \n",
    "    results.append(r[\"test_metrics\"])\n",
    "    q_results.append(r[\"test_query_level_metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T00:49:09.093149Z",
     "start_time": "2021-03-12T00:49:08.818984Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62939c8a4fe146ee5833f48a92142e10",
     "grade": false,
     "grade_id": "cell-c4ac703bc5a62d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "compare_methods(labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T00:49:27.532899Z",
     "start_time": "2021-03-12T00:49:25.730394Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa461417560d69ebe07df21a8ad9b48e",
     "grade": false,
     "grade_id": "cell-a1ea52fda896045e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_distribution(labels, q_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6e424fb4065a175111443cfbf26c618",
     "grade": false,
     "grade_id": "cell-d6d65f1ab73cda9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, report the performance metrics for the methods (25 points):\n",
    "\n",
    "\n",
    "\n",
    "|     Model    | NDCG (s.d)| Precision@5 (s.d) | Recall@5 (s.d) |\n",
    "|:------------:|:----------:|:-----------:|:--------:|\n",
    "| Pointwise(R) |            |             |          |\n",
    "| Pointwise(C) |            |             |          |\n",
    "|   Pairwise   |            |             |          |\n",
    "|   Listwise   |            |             |          |\n",
    "\n",
    "\n",
    "\n",
    "**Rubric:** Each reported <Method, Metric> carries 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89bf20199df56a7b90fa88c132800f23",
     "grade": true,
     "grade_id": "cell-5e733f8b8649a66f",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The performance metrics for the methods are summarised in the following table:\n",
    "\n",
    "|     Model    | NDCG (s.d) | Precision@5 (s.d) | Recall@5 (s.d) |\n",
    "|:------------:|:----------:|:-----------:|:--------:|\n",
    "| Pointwise(R) |0.8719 (0.0510)|0.6786 (0.2840)|0.1485 (0.0833)|\n",
    "| Pointwise(C) |0.8241 (0.0561)|0.5761 (0.2925)|0.1232 (0.0758)|\n",
    "|   Pairwise   |0.8528 (0.0473)|0.6462 (0.2646)|0.1452 (0.0857)|\n",
    "|   Listwise   |0.8514 (0.0477)|0.6359 (0.2662)|0.1436 (0.0843)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c36dc224821c1146261aa2e8d06bd419",
     "grade": false,
     "grade_id": "cell-067c6d8584df601e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a conclusion in the next cell, considering (45 points):\n",
    "- rates of convergence\n",
    "- time complexity\n",
    "- performance wrt the 3 metrics\n",
    "- performance across queries\n",
    "- ... any other observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b8b4816351280cae45a1e498d1408fe",
     "grade": true,
     "grade_id": "cell-115db704e85b78c1",
     "locked": false,
     "points": 45,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Conclusion\n",
    "**Rates of convergence:**\n",
    "Since in the listwise method the aim is to optimize the NDCG metric, the rates of convergence for the different methods will be evaluated with regards to this ranking metric. \n",
    "\n",
    "**Time complexity and computation time:**\n",
    "In this section, all the experiments were run on the same laptop in order to compare computation time.\n",
    "\n",
    "With regards to the two pairwise methods, the sped up version of the model was indeed faster at training and evaluating the model, although the difference when training the model for 8 epochs was not as big as expected. The conventional pairwise method took 1m 46.3s to run the create_results cell, whereas the pairwise sped up method took 1m 34.9s; both with the parameters: 8 epochs to train the model and batch size of 1. Such a small difference in computation time was unexpected, since the sped up method uses an algorithm of linear complexity $ O(n) $ that scales linearly in the number of documents per query compared to the $ O(n^{2}) $ complexity of the conventional pairwise method that scales quadratically with the number of pairs per query. In fact, Burges et al. (2007) found in their experiments that the sped up method was 5.1 times faster for a single layer net and 8 times faster for a two layer net than the conventional pairwise method; yet our differences in computation time were not as significant. On the other hand, the listwise method (which has a similar time complexity to the pairwise sped up method) took 1m 19.7s to run the create_results cell, the pointwise regression method took 58.4s, and the pointwise classification method took 1m 4.78s. As such, to train the model and generate the results file for each method with the parameters given, the pointwise regression method was the fastest, whereas the regular pairwise method was the slowest. \n",
    "\n",
    "Furthermore, to train each model for 1 epoch with all of the same parameters (except for batch size = 256 for pointwise methods and batch size = 1 for the rest of the methods), the pointwise (R) method took 3.71s, the pointwise (C) method took 4.00s, the pairwise method took 10.2s, the pairwise sped up method took 9.57s, and the listwise method took 10.5s. Hence, the time to train the model for 1 epoch is very similar between both pointwise methods, whilst the pairwise and the listwise methods are similar in terms of computation time between each other but take almost 3 times longer than the pointwise methods.\n",
    "\n",
    "**Performance with regards to the 3 metrics:**\n",
    "Overall, the best performing method on the test set with regards to mean NDCG, mean precision@5, and mean recall@5 as shown on the table above is the pointwise regression method. This is unexpected since pointwise methods do not directly optimize ranking quality but rather try to predict document scores independently. The pairwise and the listwise methods perform very similarly, with the pairwise method performing only slightly better across all 3 metrics making it the second-best method. This is surprising as well since it is expected for the listwise method to perform much better than the rest because it is the only method that aims to optimize a ranking metric, NDCG in this case, directly. The worst performing method is the pointwise classification method, which is expected. Interestingly, the ranking of best to worst performing methods is the same for each of the 3 metrics considered. The pairwise method has the smallest standard deviation for NDCG and precision@5 while the pointwise classification method has the smallest standard deviation for recall@5, although all methods have standard deviations of the same order of magnitude with regards to each metric.\n",
    "\n",
    "It must be noted that the parameters given were slightly different across the different methods, which could have influenced the mean scores. In terms of epochs, the pointwise regression model was trained for 11 epochs, the pointwise classification model for 13 epochs, the pairwise sped up model for 8 epochs, and the listwise model for 8 epochs. For batch size, the pointwise methods had a batch size of 256 query-document pairs. Instead, both the pairwise and the listwise methods had a batch size of 1 query, where each query had a different number of query-document pairs depending on the query. Additionally, the learning rate for the listwise method was not 1e-3 like in the other methods but rather 1e-4. \n",
    "\n",
    "**Performance across queries:**\n",
    "By looking at the bar plots where the various methods are compared across queries with regards to the distribution of NDCG scores, it can be observed that the pairwise method gives the lowest NDCG value overall with a NDCG of 0.645, whereas the highest NDCG value is obtained by the pointwise regression method with a NDCG of 0.956. The difference between the two pointwise methods in terms of NDCG is even more pronounced across queries as shown in the pointwise (R) vs pointwise (C) distribution plot, where the lowest and highest NDCG scores for a query are 0.666 and 0.916 for the pointwise (C) method, yet 0.716 and 0.956 for the pointwise (R) method. Additionally, the pointwise regression method is the method that generates the most NDCG values over 0.9, which contributes to its mean NDCG being the highest amongst all the methods.\n",
    "\n",
    "With regards to the test precision@5 scores at query level, all the methods have values ranging from 0.0 to 1.0. The most frequent value for the pointwise (R) method is 0.6 with over 30 queries, 0.6 for the pointwise (C) method with just under 30 queries, 0.6 for the pairwise method with over 30 queries, and 0.8 for the listwise method with over 40 queries. The pairwise method is the method that has the least amount of queries with a precision@5 score of 0.0 with only 3 queries, whereas both pointwise methods have the most amount of queries with a value of 0.0 with 7 queries. The method with the most amount of queries with a value of 1.0 is the pointwise (R) method with 31 queries, whilst the listwise method has the least with only 17 test queries achieving this score.\n",
    "\n",
    "In terms of the recall@5 scores for the queries of the test dataset, the distributions are generally heterogeneous. All the methods have at least 2 queries with a score of 0.0, with both pointwise methods having the most queries (7 queries) with this score. The pairwise method has the highest recall@5 score with a score of 0.5 for 1 query.\n",
    "\n",
    "**Other observations:**\n",
    "In the conventional pairwise method, a vectorized approach is followed to make computation more efficient. However, this means that the diagonal of the resulting matrix before the mean is computed is not 0. This is because the vectorized approach does not exclude the comparison between a document and itself, and thus the term $ \\log(1+ e^{-\\sigma(s_i - s_j)}) $ is computed, which is non-zero when i=j. This is the reason why we included the following line of code before calculating the mean, since it successfully removes the diagonal: $$ loss\\_per\\_pair = loss\\_per\\_pair.masked\\_select(\\sim torch.eye(n, dtype=bool)).view(n, n - 1) $$ In the calculation of lambda_i as part of the pairwise sped up method, this additional step was not necessary since the diagonal of the matrix is always 0.\n",
    "\n",
    "In the listwise method, the ranking metric chosen to optimize was the NDCG. However, this presented an issue when all of the document-query pairs for a specific query had relevance labels of 0, which was the case for 42 queries in the training dataset. This is because $ NDCG = \\frac{DCG}{Ideal DCG (IDCG)} $, yet IDCG ends up being zero when all the ideal labels are zero, resulting in a division by zero which returns NaNs for the NDCG. As such, a very small constant epsilon was added to the IDCG value in the listwise loss function to prevent divisions by zero so that the difference in NDCG and the listwise loss can be calculated properly. Other possible alternatives to deal with this issue would have been to consider another ranking metric to optimize such as DCG, or to skip the queries for which all the relevance labels were 0. \n",
    "\n",
    "**Conclusion:**\n",
    "After analysing the mean performance with regards to 3 key metrics, the time complexity and computation time, and the performance across queries, it can be concluded that the pointwise regression method is the best-performing yet also the least time-consuming method for this specific dataset out of the various methods considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b56e576a11e8312912751b345e72bfa",
     "grade": false,
     "grade_id": "cell-1693a9c859f14127",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Chapter 2: Online LTR (180 points) <a class=\"anchor\" id=\"onLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this part we want to use user interactions for training learning to rank algorithms.\n",
    "This part consists of the following sections:\n",
    " - Clicks Simulation (15 points)\n",
    " - Counterfactual Learning to Rank (80 points)\n",
    " - Online Evaluation (50 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "676f010e1d093ec248035d00c6888bf4",
     "grade": false,
     "grade_id": "cell-cb0180fd80d4f2cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1: Clicks Simulation (15 points) <a class=\"anchor\" id=\"clicks\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In online LTR, we work with user interactions such as clicks.\n",
    "One way to test online LTR algorithms is to conduce semi-synthetic experiments.\n",
    "In semi-synthetic experiments, the feature vectors of a LTR dataset (similar to what you used in Part 1) are used.\n",
    "But instead of using the relevance labels for training the LTR algorithm, **simulated clicks** are used.\n",
    "\n",
    "In this section we want to simulate clicks based on the labels and use the generated clicks in later sections to train our Counterfactual LTR (CLTR) algorithm.\n",
    "\n",
    "First we need to have a *production ranker* that determines the order of documents for each query.\n",
    "The click probability of each document depends on its rank in the results list as well as its relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efe77df779addec15df70a2bd8a7aaee",
     "grade": false,
     "grade_id": "cell-4ae04e0717236f54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranks has 3 keys: dict_keys(['train', 'valid', 'test'])\n",
      "train shape:(85227,)\n",
      "valid shape:(12794,)\n",
      "test shape:(29881,)\n"
     ]
    }
   ],
   "source": [
    "ranks = dataset.load_production_ranker()\n",
    "print(f'ranks has {len(ranks.keys())} keys: {ranks.keys()}')\n",
    "for key in ranks:\n",
    "    print(f'{key} shape:{ranks[key].shape}')\n",
    "    \n",
    "data.train.ranks = ranks['train']\n",
    "data.validation.ranks = ranks['valid']\n",
    "data.test.ranks = ranks['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc0ae9b0654edea0e837893b613af5b8",
     "grade": false,
     "grade_id": "cell-86dfb91f27aeb1cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (5 points):**\n",
    "Now use the `ranks` to generate clicks.\n",
    "We assume documents with label $[3,4]$ are relevant and labels $[0,1,2]$ are non-relevant.\n",
    "We also assume the examination probability of a document at rank $r$ is $\\frac{1}{r}$.\n",
    "Finally, we assume a $0.05$ noise.\n",
    "\n",
    "This means that the click probability of a document with label $[3,4]$ at rank $r$ is $\\frac{1}{r}$, while the click probability of a document with label $[0,1,2]$ at rank $r$ is $\\frac{0.05}{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bdfcf8895e430580912b057991ac7c0",
     "grade": false,
     "grade_id": "cell-6302b18566bcece2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def generate_clicks(ranking, query_labels):\n",
    "    \"\"\"\n",
    "    Generates random clicks based on the given list of ranking and relevance labels for the documents of one query.\n",
    "    Input:\n",
    "        ranking: contains the rank of documents, eg. ranking[5]=0 means that document 5 is shown at rank 0.\n",
    "        query_labels: contains the labels of documents, eg. query_labels[5]=3 means that document 5 has a label equal to 3.\n",
    "    Output:\n",
    "        A np.array of clicked positions, eq. clicked=[1,4] meanse that documents at ranks 1 and 4 have been clicked. Note that document at rank 1 may differ from document 1 (see description of ranking input).\n",
    "        \n",
    "    Hint: Use the np.random.binomial function for generating click from probability.\n",
    "    \"\"\"\n",
    "    theta = 1. / np.arange(1, ranking.shape[0] + 1)\n",
    "   \n",
    "    noise_prob = 0.05 * theta\n",
    "    # YOUR CODE HERE\n",
    "    zips = list(zip(ranking, query_labels))\n",
    "\n",
    "    zips.sort(key=lambda x:x[0])\n",
    "    final_prob = list()\n",
    "    for loc, (x, y) in enumerate(zips):\n",
    "        if y in [0,  1, 2]:\n",
    "            final_prob.append(noise_prob[x])\n",
    "        else:\n",
    "            final_prob.append(theta[x])\n",
    "  \n",
    "    mask = np.random.binomial(1, final_prob, size=len(ranking))\n",
    "\n",
    "    clicked = [elem for keep, elem in zip(mask, np.sort(ranking)) if keep]\n",
    "    return np.array(clicked).astype(np.int32)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d5eeabc53fd3a13bb33d46f631ab3d5",
     "grade": true,
     "grade_id": "cell-098685a98a1887f4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks: [0 1 6]\n",
      "expected clicks for seed 4: [0 1 6]\n"
     ]
    }
   ],
   "source": [
    "ranking = np.array([2, 1, 3, 0, 4, 7, 5, 6])\n",
    "labels = np.array([0, 3, 4, 1, 1, 2, 4, 3])\n",
    "\n",
    "np.random.seed(4)\n",
    "print(f'clicks: {generate_clicks(ranking, labels)}')\n",
    "print('expected clicks for seed 4: [0 1 6]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fdf6ed2c6545213fb432e06dafb088e",
     "grade": false,
     "grade_id": "cell-a45b9902b9ff34c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (10 points):**\n",
    "Use `generate_clicks` to simulate clicks for different query sessions.\n",
    "For this, randomly select a query id, generate random clicks on documents of that query using your `generate_clicks` implementation and only keep the clicks on `topk` positions (i.e. do not consider clicks on ranks after `topk` rank).\n",
    "Repeat this process until you have `click_count` number of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea8a1c5964a999322c4b30bdeecf88e6",
     "grade": false,
     "grade_id": "cell-cef4496878e0428a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def simulate_clicks(data, split_name, click_count, topk=20):\n",
    "    \"\"\"\n",
    "    Simulate click_count clicks on topk results on the specified split of the data.\n",
    "    Output:\n",
    "        A list L=[L_0, L_1, ..., L_n] of lists. Each L_i is itself a list of clicks over the documents of query number i, eg. L_0=[0,2,0,1,0,5,2] means that in the sessions of the first query, the documents at positions 0,1,2 and 5 were clicked 3,1,2 and 1 times, respectively.\n",
    "    \"\"\"\n",
    "    data_split = getattr(data, split_name)\n",
    "    ranks = data_split.ranks\n",
    "    #argsorted, doclist_ranges = dataset.clean_and_sort(ranks, data_split, topk)\n",
    "\n",
    "    #click_data_split = Namespace(feature_matrix = data_split.feature_matrix[argsorted,:],\n",
    "                                 #doclist_ranges = doclist_ranges,\n",
    "                                 #clicks = None)\n",
    "    #labels = data_split.label_vector[argsorted]                               \n",
    "    #num_queries = click_data_split.doclist_ranges.shape[0] - 1\n",
    "    num_queries = data_split.doclist_ranges.shape[0] - 1\n",
    "    clicks = [[] for _ in range(num_queries)]\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    num_clicks = 0\n",
    "    while num_clicks<=click_count:\n",
    "       \n",
    "        query_id = np.random.randint(num_queries)\n",
    "      \n",
    "        query_labels = data_split.query_labels(query_id)\n",
    "        ranks_query = ranks[data_split.doclist_ranges[query_id]:data_split.doclist_ranges[query_id+1]]\n",
    "        inverse_ranking = np.argsort(ranks_query)\n",
    "        generated_clicks = generate_clicks(inverse_ranking, query_labels)\n",
    "      \n",
    "        generated_clicks =  generated_clicks[generated_clicks<topk]\n",
    "    \n",
    "        num_clicks+=len(generated_clicks)\n",
    "        clicks[query_id].extend(list(generated_clicks))\n",
    "  \n",
    "\n",
    "    return clicks   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "257a37637e4baec5b5da06cfb28f9c10",
     "grade": false,
     "grade_id": "cell-d31c106145d57d5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use `simulate_clicks` function to simulate clicks for train and validation sets.\n",
    "\n",
    "**Note:**\n",
    "You can dump your generated clicks using pickle or numby and load them in your future runs to avoid long waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe427c2102d2999e8bd2a6243c0d18b6",
     "grade": true,
     "grade_id": "cell-c7adb3055f3a3d5c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001 clicks simulated over 2735 queries.\n",
      "On average, each session has 3.656672760511883 clicks.\n",
      "min number of clicks per session:0\n",
      "max number of clicks per session:43\n"
     ]
    }
   ],
   "source": [
    "train_clicks = simulate_clicks(data,'train', click_count = 10000)\n",
    "\n",
    "assert isinstance(train_clicks, list)\n",
    "session_clicks = np.array(list(map(len, train_clicks)))\n",
    "print(f'{session_clicks.sum()} clicks simulated over {session_clicks.shape[0]} queries.')\n",
    "print(f'On average, each session has {session_clicks.mean()} clicks.')\n",
    "print(f'min number of clicks per session:{session_clicks.min()}')\n",
    "print(f'max number of clicks per session:{session_clicks.max()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87c03dbedf499cbb4ce724f2e97dc8aa",
     "grade": false,
     "grade_id": "cell-f6996aa4808d7747",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For our new click data, we need a new dataloader.\n",
    "`QueryGroupedOnlineLTRData` provides what you will need for training an LTR model from the clicks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da61ee98e6094a5bf41deff34136be4e",
     "grade": false,
     "grade_id": "cell-036f66cea8bae0bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QueryGroupedOnlineLTRData(Dataset):\n",
    "    def __init__(self, data, split, clicks, topk):\n",
    "        self.split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert self.split is not None, \"Invalid split!\"\n",
    "        self.clicks = clicks\n",
    "        self.topk = topk\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.split.num_queries()\n",
    "\n",
    "    def __getitem__(self, q_i):\n",
    "        s_i = self.split.doclist_ranges[q_i]\n",
    "        e_i = self.split.doclist_ranges[q_i + 1]\n",
    "        feature_ = self.split.feature_matrix[s_i:e_i,:]\n",
    "        ranking = self.split.ranks[s_i:e_i]\n",
    "        inverse_ranking = np.argsort(ranking)\n",
    "#       We re-order the rows of feature matrix so that in coincides with the ranking of documents.\n",
    "#       We also cut the documents after rank topk.\n",
    "        feature = torch.FloatTensor(feature_[inverse_ranking[:self.topk],:])\n",
    "        clicks = self.clicks[q_i]\n",
    "        return q_i, feature, clicks\n",
    "    \n",
    "def qgo_collate_fn(batch):\n",
    "    \n",
    "    qids = []\n",
    "    features = []\n",
    "    clicks = []\n",
    "    \n",
    "    for (q, f, l) in batch:\n",
    "        qids.append(q)\n",
    "        features.append(f)\n",
    "        clicks.append(l)\n",
    "    return qids, features, clicks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33284e30370a8dfdd116267499660b27",
     "grade": false,
     "grade_id": "cell-3ff364596e375e19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Counterfactual LTR (90 points) <a class=\"anchor\" id=\"cLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Now we want to use the simulated clicks and train a counterfactual LTR algorithm.\n",
    "We will use a simple loss function:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} r(d, q) \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $Q$ is the set of queries, $D_q$ is the list of documents shown to the user for query $q$, $r(d, q)$ is the relevance of document $d$ to query $q$ and $rank(d, D_q)$ is the rank of $d$ in $D_q$.\n",
    "\n",
    "In this assignment, we use sigmoid-like bound for $\\lambda(\\bar{rank}(d, D_q))$:\n",
    "\n",
    "$$\n",
    "\\lambda(\\bar{rank}(d, D_q)) = \\sum_{d' \\in D_q} log\\left(\n",
    "1+exp\\left(-2\\cdot(f(d)-f(d')\\right)\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a52232af5a6b442146258942ced99353",
     "grade": false,
     "grade_id": "cell-5d364d940f11536e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.1. Biased LTR (20 points)\n",
    "\n",
    "Remember that we do not have the explicit relevance in the online settings.\n",
    "Instead, we have to learn from the clicks.\n",
    "Clicks, implicitly indicate relevance.\n",
    "A naive approach for learning from clicks would be to replace relevance with clicks and train a model with the loss function:\n",
    "\n",
    "$$\n",
    "L_{biased}=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} c(d, q) \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $c(d, q)$ indicates the number of times $d$ was clicked for query $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document with no clicks???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "defc1880152e68dd36c21992263f8d56",
     "grade": false,
     "grade_id": "cell-6b45762861989c96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def online_loss_biased(scores, clicks):\n",
    "    \"\"\"\n",
    "    Compute and return the biased online loss *for a single query*. To compute this, use L_{biased} formula above.\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    clicks: list of clicked ranks. \n",
    "    \n",
    "    Note: the scores are aligned with the click positions, i.e. scores[0] corresponds to the 0 rand, etc.\n",
    "    \n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    #click_counts = Counter(clicks)\n",
    "    click_counts = torch.bincount(torch.tensor(clicks), minlength=scores.shape[0])\n",
    "    diff = scores - torch.transpose(scores, 0, 1)\n",
    "    lambda_rank = torch.log(1 + torch.exp(-2 * diff))\n",
    "    lambda_rank = lambda_rank.sum(dim=1)\n",
    "    final_loss = click_counts*lambda_rank\n",
    "    final_loss = final_loss.sum()\n",
    "    return final_loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd1acc24a7415fcae69fa183a51afb28",
     "grade": true,
     "grade_id": "cell-d09e7eb339052376",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased loss: 139.16244506835938\n",
      "expected: -6.205410003662109\n"
     ]
    }
   ],
   "source": [
    "scores = torch.FloatTensor([1.2, -2, 1.1, 2.4, 0, 1.2])[:,None]\n",
    "clicks = [0, 0, 1, 3, 1, 4, 0, 5, 0, 1, 2]\n",
    "print(f'biased loss: {online_loss_biased(scores, clicks)}')\n",
    "print(f'expected: -6.205410003662109')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48efe214b4ffd699be8ebbbf0016b467",
     "grade": false,
     "grade_id": "cell-318d7a3b824c5f99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.2. Unbiased LTR (10 points)\n",
    "Now modify `online_loss_biased` to debias the clicks, using IPS method:\n",
    "\n",
    "$$\n",
    "L_{unbiased}=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} \\frac{c(d, q)}{P\\left(o(d,q)=1\\right)} \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $P\\left(o(d,q)=1\\right)$ indicates the probability that $d$ was observed by the user for query $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e0238d2d1cabe1a3d21b0cc44a54d61",
     "grade": false,
     "grade_id": "cell-9362e909abcb1d91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def online_loss_unbiased(scores, clicks):\n",
    "    \"\"\"\n",
    "    Compute and return the unbiased online loss *for a single query*. To compute this, use L_{unbiased} formula above.\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    clicks: list of clicked ranks. \n",
    "    \n",
    "    Note 1: the scores are aligned with the click positions, i.e. scores[0] corresponds to the 0 rand, etc.\n",
    "    Note 2: the weights are provided.\n",
    "    \n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    #click_counts = Counter(clicks)\n",
    "    weights = torch.Tensor((1./np.arange(1,scores.shape[0]+1))[:,None])\n",
    "    click_counts = torch.bincount(torch.tensor(clicks), minlength=scores.shape[0])\n",
    "    diff = scores - torch.transpose(scores, 0, 1)\n",
    "    lambda_rank = torch.log(1 + torch.exp(-2 * diff))\n",
    "    lambda_rank = lambda_rank.sum(dim=1)\n",
    "    final_loss = click_counts*lambda_rank/torch.squeeze(weights, dim=1)\n",
    "    final_loss = final_loss.sum()       \n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2bcac0a6c4afe70fca335c8d648122f",
     "grade": true,
     "grade_id": "cell-30b468279fbb6de5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased loss: 323.6814270019531\n",
      "expected: -17.11062240600586\n"
     ]
    }
   ],
   "source": [
    "scores = torch.FloatTensor([1.2, -2, 1.1, 2.4, 0, 1.2])[:,None]\n",
    "clicks = [0, 0, 1, 3, 1, 4, 0, 5, 0, 1, 2]\n",
    "print(f'biased loss: {online_loss_unbiased(scores, clicks)}')\n",
    "print(f'expected: -17.11062240600586')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4f49c8953f6080ec340ebe076f9caf6",
     "grade": false,
     "grade_id": "cell-e385419532bb6535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (60 points):**\n",
    "In the next cell, write a wrapper that uses your loss functions and simulated clicks to train an online LTR method (no evaluation on validation set is required):\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected for biased loss: 15 points\n",
    " - Performance as expected for unbiased loss: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1113e39b6c82d64e959436f049df22ac",
     "grade": false,
     "grade_id": "cell-155a60a13f0dabb3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (60 points)\n",
    "def train_online(net, train_clicks, loss_fn, params):\n",
    "    \"\"\"\n",
    "    Use QueryGroupedOnlineLTRData to load the data train split and clicks.\n",
    "    Use the appropriate loss_fn (biased/unbiased).\n",
    "    No need to use validation set.\n",
    "    \"\"\"\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    train_dl = DataLoader(QueryGroupedOnlineLTRData(data, \"train\", train_clicks, params.topk), batch_size=1, shuffle=True, collate_fn=qgo_collate_fn)\n",
    "    optimiser = Adam(net.parameters(), lr=params.lr)\n",
    "    net.train()\n",
    "    for i in range(params.epochs):\n",
    "        for (qids, x, y) in train_dl:\n",
    "             for q_i, features_i, clicks_i in zip(qids, x, y):\n",
    "                    if len(clicks_i)==0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        optimiser.zero_grad()\n",
    "                        outputs = net(features_i)\n",
    "                        loss_value = loss_fn(outputs, clicks_i)\n",
    "                        loss_value.backward()\n",
    "                        optimiser.step()\n",
    "                    \n",
    "        dict_epoch_train = {}\n",
    "        dict_values_train = evaluate_model(net, \"train\", print_results=False)\n",
    "        if hasattr(params, 'metrics'):\n",
    "            select_m_train = {k: v for (k, v) in dict_values_train.items() if k in params.metrics}\n",
    "            dict_epoch_train[f'Epoch {i+1}'] = select_m_train\n",
    "        else:\n",
    "            dict_epoch_train[f'Epoch {i+1}'] = dict_values_train\n",
    "        train_metrics_epoch.append(dict_epoch_train)\n",
    "        \n",
    "    return {\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "688a802dd604c3f002a9c5b26822b42d",
     "grade": false,
     "grade_id": "cell-2899cebacae54c00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'metrics_train': [{'Epoch 1': {'dcg': (101.30271531631529,\n",
       "     24.690688394367584),\n",
       "    'dcg@03': (16.333697738084734, 8.002037601794827),\n",
       "    'dcg@05': (20.90144867737567, 9.259207779794961),\n",
       "    'dcg@10': (27.89566234655448, 11.157160668390077),\n",
       "    'dcg@20': (36.79813809325824, 12.858132859834997),\n",
       "    'ndcg': (0.8154777527678491, 0.05861369139374421),\n",
       "    'ndcg@03': (0.5726419995675828, 0.2414044451826928),\n",
       "    'ndcg@05': (0.5539602049993165, 0.1962691572153872),\n",
       "    'ndcg@10': (0.5419229791663889, 0.16156685926933406),\n",
       "    'ndcg@20': (0.5487227095580517, 0.13400403449360576),\n",
       "    'precision@01': (0.6666666666666666, 0.4714045207910317),\n",
       "    'precision@03': (0.5615615615615616, 0.3088570704829089),\n",
       "    'precision@05': (0.4984984984984985, 0.2605679247166445),\n",
       "    'precision@10': (0.4069069069069069, 0.21106133997388984),\n",
       "    'precision@20': (0.32852852852852854, 0.16241402418568762),\n",
       "    'recall@01': (0.034893468832485675, 0.03835711445294366),\n",
       "    'recall@03': (0.08530406901056486, 0.06890427525458578),\n",
       "    'recall@05': (0.12204201487438122, 0.08559946444460198),\n",
       "    'recall@10': (0.19202158617133053, 0.11425979343674421),\n",
       "    'recall@20': (0.2978938209147964, 0.13717298764800898),\n",
       "    'relevant rank': (72.45025913285299, 65.05887785525668),\n",
       "    'relevant rank per query': (1721.1831831831832, 1007.792091841296)}},\n",
       "  {'Epoch 2': {'dcg': (103.68269950320125, 25.021510079964646),\n",
       "    'dcg@03': (18.186054846943588, 8.49893954376115),\n",
       "    'dcg@05': (22.892050483114133, 9.731386424192308),\n",
       "    'dcg@10': (30.495998855041105, 11.646197886458406),\n",
       "    'dcg@20': (39.707227850474645, 13.333152306245722),\n",
       "    'ndcg': (0.8348526932778788, 0.056798315155207196),\n",
       "    'ndcg@03': (0.6339957511207119, 0.25021250126894895),\n",
       "    'ndcg@05': (0.6048117356873468, 0.20092803106077675),\n",
       "    'ndcg@10': (0.5917754981065163, 0.1615794957087604),\n",
       "    'ndcg@20': (0.5914542810807544, 0.13082705123848262),\n",
       "    'precision@01': (0.7477477477477478, 0.434305253808791),\n",
       "    'precision@03': (0.6246246246246246, 0.3135802398127715),\n",
       "    'precision@05': (0.5453453453453453, 0.2626115394961394),\n",
       "    'precision@10': (0.4495495495495495, 0.2103367272640074),\n",
       "    'precision@20': (0.3512012012012012, 0.15952745082312536),\n",
       "    'recall@01': (0.03826634290401641, 0.03702228631449562),\n",
       "    'recall@03': (0.09385863954655084, 0.06818338940245751),\n",
       "    'recall@05': (0.13260088760896246, 0.08540111870101688),\n",
       "    'recall@10': (0.21283140434506678, 0.11871516093565462),\n",
       "    'recall@20': (0.3208707726687298, 0.1394035626065092),\n",
       "    'relevant rank': (66.64821135128302, 61.41807344911029),\n",
       "    'relevant rank per query': (1583.3453453453453, 991.5177665262448)}},\n",
       "  {'Epoch 3': {'dcg': (101.09114425347948, 24.580454914082587),\n",
       "    'dcg@03': (15.640659368350697, 8.388702307630524),\n",
       "    'dcg@05': (19.954860144484798, 9.705351143128274),\n",
       "    'dcg@10': (26.679670476427233, 11.1367914827145),\n",
       "    'dcg@20': (35.882380299556075, 12.865980650684527),\n",
       "    'ndcg': (0.8139916600418052, 0.06086421911920661),\n",
       "    'ndcg@03': (0.5474765448552675, 0.25993979314024346),\n",
       "    'ndcg@05': (0.5295332092814742, 0.21557582249791496),\n",
       "    'ndcg@10': (0.5191503807614376, 0.1681823326889224),\n",
       "    'ndcg@20': (0.5350737377729736, 0.13523811532291954),\n",
       "    'precision@01': (0.6366366366366366, 0.4809682209134276),\n",
       "    'precision@03': (0.5405405405405406, 0.3319914553637528),\n",
       "    'precision@05': (0.4780780780780781, 0.2718525587954499),\n",
       "    'precision@10': (0.3867867867867868, 0.20031403170243084),\n",
       "    'precision@20': (0.3153153153153153, 0.1544329148882341),\n",
       "    'recall@01': (0.031121537455084787, 0.035749120366725215),\n",
       "    'recall@03': (0.07868198823540799, 0.06023096204677223),\n",
       "    'recall@05': (0.11518659289759214, 0.07733894297999155),\n",
       "    'recall@10': (0.18066035296624527, 0.1039181381572756),\n",
       "    'recall@20': (0.2865190648127091, 0.13020992205033202),\n",
       "    'relevant rank': (70.6838579193528, 62.59444471150607),\n",
       "    'relevant rank per query': (1679.2192192192192, 1045.827394903634)}}]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = Namespace(epochs=3, lr=1e-3, topk=20)\n",
    "train_clicks = simulate_clicks(data,'train', click_count = 50000)\n",
    "biased_net = NeuralModule(1)\n",
    "train_online(biased_net, train_clicks, online_loss_biased, params)\n",
    "unbiased_net = NeuralModule(1)\n",
    "train_online(unbiased_net, train_clicks, online_loss_unbiased, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff8105a6912656bef1bf2642a977fee",
     "grade": false,
     "grade_id": "cell-0511eb2be587f032",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 105.5632 (24.21010)\n",
      "dcg@03: 16.0035 (8.88010)\n",
      "dcg@05: 20.3449 (9.99195)\n",
      "dcg@10: 27.8914 (11.95779)\n",
      "dcg@20: 36.9506 (13.24368)\n",
      "ndcg: 0.8133 (0.05939)\n",
      "ndcg@03: 0.5428 (0.27730)\n",
      "ndcg@05: 0.5229 (0.22488)\n",
      "ndcg@10: 0.5266 (0.18336)\n",
      "ndcg@20: 0.5349 (0.14579)\n",
      "precision@01: 0.6410 (0.47970)\n",
      "precision@03: 0.5356 (0.36711)\n",
      "precision@05: 0.4701 (0.28920)\n",
      "precision@10: 0.4060 (0.22732)\n",
      "precision@20: 0.3282 (0.16582)\n",
      "recall@01: 0.0285 (0.02792)\n",
      "recall@03: 0.0685 (0.05550)\n",
      "recall@05: 0.0992 (0.06836)\n",
      "recall@10: 0.1720 (0.10499)\n",
      "recall@20: 0.2726 (0.12710)\n",
      "relevant rank: 74.2760 (64.67022)\n",
      "relevant rank per query: 1911.4957 (1112.34491)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 107.3756 (23.96603)\n",
      "dcg@03: 17.2417 (8.09782)\n",
      "dcg@05: 21.9847 (9.09927)\n",
      "dcg@10: 29.4752 (11.04499)\n",
      "dcg@20: 39.4902 (12.84814)\n",
      "ndcg: 0.8286 (0.05512)\n",
      "ndcg@03: 0.5920 (0.25000)\n",
      "ndcg@05: 0.5737 (0.20426)\n",
      "ndcg@10: 0.5628 (0.16179)\n",
      "ndcg@20: 0.5759 (0.13523)\n",
      "precision@01: 0.6496 (0.47710)\n",
      "precision@03: 0.6011 (0.32097)\n",
      "precision@05: 0.5385 (0.26078)\n",
      "precision@10: 0.4470 (0.21665)\n",
      "precision@20: 0.3748 (0.17670)\n",
      "recall@01: 0.0305 (0.03178)\n",
      "recall@03: 0.0799 (0.05406)\n",
      "recall@05: 0.1207 (0.07781)\n",
      "recall@10: 0.1885 (0.09358)\n",
      "recall@20: 0.3076 (0.12826)\n",
      "relevant rank: 69.3922 (63.83305)\n",
      "relevant rank per query: 1785.8120 (1068.24846)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dcg': (107.3755766566305, 23.966025412723805),\n",
       " 'dcg@03': (17.2416756850735, 8.097824289320771),\n",
       " 'dcg@05': (21.98472053073246, 9.099265513323543),\n",
       " 'dcg@10': (29.475190008225493, 11.044989521556142),\n",
       " 'dcg@20': (39.490243722749824, 12.8481360932013),\n",
       " 'ndcg': (0.828613305707973, 0.05511994865551042),\n",
       " 'ndcg@03': (0.5919518940754398, 0.2499954309282962),\n",
       " 'ndcg@05': (0.5736646035817021, 0.20426493919666444),\n",
       " 'ndcg@10': (0.562813486008474, 0.16179175028891638),\n",
       " 'ndcg@20': (0.5758923975012932, 0.13522560122226412),\n",
       " 'precision@01': (0.6495726495726496, 0.4771037858787305),\n",
       " 'precision@03': (0.6011396011396011, 0.32096517619390674),\n",
       " 'precision@05': (0.5384615384615384, 0.2607832232839204),\n",
       " 'precision@10': (0.447008547008547, 0.21664601462626626),\n",
       " 'precision@20': (0.3747863247863248, 0.17670100435819167),\n",
       " 'recall@01': (0.03045598375275576, 0.031784104653490935),\n",
       " 'recall@03': (0.0798674374049898, 0.054063929286149316),\n",
       " 'recall@05': (0.12070336487514428, 0.077813089253637),\n",
       " 'recall@10': (0.18853672682702438, 0.093580856408595),\n",
       " 'recall@20': (0.30764839899404794, 0.12825897613774787),\n",
       " 'relevant rank': (69.39222849551643, 63.833054475109655),\n",
       " 'relevant rank per query': (1785.8119658119658, 1068.248458019925)}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(biased_net, 'test', print_results=True)\n",
    "evaluate_model(unbiased_net, 'test', print_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a5138ee2bdc3b5682d5ed8b7bcc1202",
     "grade": false,
     "grade_id": "cell-c497608f3d46e769",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 104.5344 (23.95721)\n",
      "dcg@03: 14.5303 (8.31687)\n",
      "dcg@05: 18.4200 (9.39325)\n",
      "dcg@10: 26.2828 (11.34135)\n",
      "dcg@20: 35.7106 (12.91279)\n",
      "ndcg: 0.8063 (0.06201)\n",
      "ndcg@03: 0.4977 (0.26592)\n",
      "ndcg@05: 0.4805 (0.22176)\n",
      "ndcg@10: 0.5030 (0.18779)\n",
      "ndcg@20: 0.5208 (0.14863)\n",
      "precision@01: 0.5214 (0.49954)\n",
      "precision@03: 0.4843 (0.32757)\n",
      "precision@05: 0.4359 (0.27502)\n",
      "precision@10: 0.4026 (0.22474)\n",
      "precision@20: 0.3338 (0.16170)\n",
      "recall@01: 0.0232 (0.02965)\n",
      "recall@03: 0.0647 (0.05122)\n",
      "recall@05: 0.0960 (0.07677)\n",
      "recall@10: 0.1732 (0.11692)\n",
      "recall@20: 0.2763 (0.12301)\n",
      "relevant rank: 73.3590 (64.63972)\n",
      "relevant rank per query: 1887.8974 (1062.99060)\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "train_clicks = simulate_clicks(data,'train', click_count = 50000)\n",
    "params = Namespace(epochs=3, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    topk=20,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "biased_model = NeuralModule(1)\n",
    "\n",
    "create_results(biased_model, \n",
    "               train_online, \n",
    "               biased_model,\n",
    "               \"./biased_model.json\",\n",
    "               train_clicks,\n",
    "               online_loss_biased,\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(biased_model.state_dict(), \"./biased_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9b104b0cc6204f1f3c6ac9bbf88171f",
     "grade": false,
     "grade_id": "cell-9df7065c5b887449",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 107.6447 (24.22702)\n",
      "dcg@03: 17.5477 (8.44262)\n",
      "dcg@05: 22.5764 (9.22815)\n",
      "dcg@10: 30.2939 (11.29585)\n",
      "dcg@20: 39.9549 (13.42362)\n",
      "ndcg: 0.8304 (0.05803)\n",
      "ndcg@03: 0.5988 (0.25801)\n",
      "ndcg@05: 0.5858 (0.20113)\n",
      "ndcg@10: 0.5789 (0.16907)\n",
      "ndcg@20: 0.5812 (0.14303)\n",
      "precision@01: 0.7094 (0.45404)\n",
      "precision@03: 0.6068 (0.33080)\n",
      "precision@05: 0.5504 (0.26655)\n",
      "precision@10: 0.4590 (0.22078)\n",
      "precision@20: 0.3679 (0.17103)\n",
      "recall@01: 0.0315 (0.02802)\n",
      "recall@03: 0.0801 (0.05420)\n",
      "recall@05: 0.1201 (0.07367)\n",
      "recall@10: 0.1957 (0.10212)\n",
      "recall@20: 0.3059 (0.12994)\n",
      "relevant rank: 69.4062 (63.21880)\n",
      "relevant rank per query: 1786.1709 (1026.13405)\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "\n",
    "params = Namespace(epochs=3, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    topk=20,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "unbiased_model = NeuralModule(1)\n",
    "\n",
    "create_results(unbiased_model, \n",
    "               train_online, \n",
    "               unbiased_model,\n",
    "               \"./unbiased_model.json\",\n",
    "               train_clicks,\n",
    "               online_loss_unbiased,\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(biased_model.state_dict(), \"./unbiased_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ede3bc5a61edfb93777647c3ae0b1a",
     "grade": true,
     "grade_id": "cell-5a98cb1d4dccab54",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fad8b3eebaaca7a38cf37fb55433e3",
     "grade": true,
     "grade_id": "cell-6eaff35ed2f350ea",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8cc768ddc4dd12732fc4c1b42cf015d",
     "grade": true,
     "grade_id": "cell-6a1e7d45a6b24c7c",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca775c32f1df7ef6d70c60b3e365adf4",
     "grade": false,
     "grade_id": "cell-08980b08558f70e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.3: Comparison (20 points)\n",
    "\n",
    "Now we can compare two loss functions: biased and unbiased.\n",
    "We want to see how they can improve by increasing the number of training clicks.\n",
    "Train both biased and unbaised models on $[2000, 10000, 50000]$ number of clicks and compare the results.\n",
    "\n",
    "Plot the results in a **single** figure with x-axis showing the number of trainin clicks and y-axis showing the nDCG@10.\n",
    "Discuss your observations.\n",
    "\n",
    "\n",
    "**Rubric:**\n",
    "- Two curves are plotted in the figure: 10 points\n",
    "- Clear titles, x label, y labels and legends (if applicable): 5 points\n",
    "- Explain what you observe: 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76693360dd6493e5811d1908eacf8227",
     "grade": true,
     "grade_id": "cell-f51a0be2734c9352",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following we will evaluate the nDCG@10 scores for different total number of training clicks: 2000, 10000, 50000. Therefore, we use bar plots to display and compare the scores when using a biased loss and using an unbiased. As expected higher scorescoincide to higher number of training clicks. This is due to the fact that the existing models assume sufficient amount of click datato reliably train the parameters of the network. Looking at the final values for the scores, one can conclude that even 50000 clicks it is not enough for convergence. We can then conclude that  the approaches are dependent on the number of clicks simulated. In terms of performance, when we have 2000 cliks we can see similar performance for both biased and unbiased loss. Then, we can see that the unbiased loss outperforms the biased loss. This is expectedconsidering that the unbiased loss provides an unbiased estimator for the IR metric by using simulated clicks. The biased estimator is biased by the examinationprobability of a document being examined. Unbiased loss alleviates this issue by dividing by this probability.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "def autolabel(val):\n",
    "    for i in val:\n",
    "        height = round(i.get_height(), 3)\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(i.get_x() + i.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "seed(8)\n",
    "max_clicks = [2000, 10000, 50000]\n",
    "\n",
    "unbiased = list()\n",
    "biased = list()\n",
    "params = Namespace(epochs=3, lr=1e-3, topk=20, metrics={\"ndcg@10\"})\n",
    "for i in max_clicks:\n",
    "    train_clicks = simulate_clicks(data,'train', click_count = i)\n",
    "    biased_net = NeuralModule(1)\n",
    "    train_online(biased_net, train_clicks, online_loss_biased, params)\n",
    "    unbiased_net = NeuralModule(1)\n",
    "    train_online(unbiased_net, train_clicks, online_loss_unbiased, params)\n",
    "    value_biased=evaluate_model(biased_net, 'test', print_results=False)[\"ndcg@10\"]\n",
    "    value_unbiased=evaluate_model(unbiased_net, 'test', print_results=False)[\"ndcg@10\"]\n",
    "    unbiased.append(value_unbiased[0])\n",
    "    biased.append(value_biased[0])\n",
    "labels = list(map(str, max_clicks))\n",
    "label_loc = np.arange(len(labels))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "loss1 = ax.bar(label_loc - width/2, biased, width,  label='Biased loss')\n",
    "loss2 = ax.bar(label_loc + width/2, unbiased, width,  label='Unbiased loss')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_xlabel('Training clicks')\n",
    "ax.set_title('nDCG@10 scores for different numbers of training clicks')\n",
    "ax.set_xticks(label_loc)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "autolabel(loss1)\n",
    "autolabel(loss2)\n",
    "plt.show()\n",
    "\n",
    "print(\"In the following plot,  we will evaluate the nDCG@10 scores for different total number of training clicks: 2000, 10000, 50000. \\\n",
    "Therefore, we use bar plots to display and compare the scores when using a biased loss and using an unbiased one. As expected higher scores \\\n",
    "coincide to higher number of training clicks. This is due to the fact that the existing models assume sufficient amount of click data \\\n",
    "to reliably train the parameters of the network. Looking at the final values for the scores, one can conclude that even 50000 clicks it is \\\n",
    "not enough for convergence. We can then conclude that  the approaches are dependent on the number of clicks simulated. In terms of performance, \\\n",
    "when we have 2000 cliks we can see similar performance for both biased and unbiased loss. Then, we can see that the unbiased loss outperforms the biased loss. This is expected \\\n",
    "considering that the unbiased loss provides an unbiased estimator for the IR metric by using simulated clicks. The biased estimator is biased by the examination \\\n",
    "probability of a document being examined. Unbiased loss alleviates this issue by dividing by this probability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce47393cb34036b50fde6780cdb9f772",
     "grade": false,
     "grade_id": "cell-6bd0ff7cecc1b060",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3: Online Evaluation (75 points) <a class=\"anchor\" id=\"on_eval\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Sometimes, in online search engines, we want to compare two or more different ranking functions based on the user interactions.\n",
    "This comparison is done via online evaluation.\n",
    "\n",
    "In this section we implement one of the online evaluation methods: probabilistic multileaving.\n",
    "\n",
    "We compare three rankers:\n",
    " - Production ranker: the ranks that are provided to you for doing the click simulation.\n",
    " - Biased method from previous section.\n",
    " - Unbiased method from previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "590bdae1272cdd45e304d0498de730bc",
     "grade": false,
     "grade_id": "cell-6ca9cf6f08e6b7f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before proceeding, we need some auxiliary functions:\n",
    "\n",
    " - `invert_ranking` gets the ranking and gives the inverted rankings. This means that for an input with `rank[d]=r`, the output would be `i_rank[r]=d`.\n",
    "\n",
    "\n",
    " - `get_predictions` gives a dictionary of predictions, i.e. `predictions[qid]` is the list of scores produced by the given model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98f77357b3d3088c738c97a90bd05061",
     "grade": false,
     "grade_id": "cell-3ae14fda3426a247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(net, test_clicks, topk):\n",
    "    test_dl = DataLoader(QueryGroupedOnlineLTRData(data, 'test', test_clicks, topk), \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          collate_fn=qg_collate_fn)\n",
    "    predictions = {}\n",
    "    for qids, x, _ in test_dl:\n",
    "        predictions[qids[0]] = net(x[0]).detach().numpy()[:,0]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def invert_rankings(rankings):\n",
    "    '''\n",
    "    Invert indices in a matrix of rankings, ranking per row.\n",
    "    '''\n",
    "    inverted = np.zeros(rankings.shape)\n",
    "    \n",
    "    inverted[np.arange(rankings.shape[0])[:,None],rankings] = np.arange(rankings.shape[1])[None,:]\n",
    "    return inverted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "715b799e36aa757ed9db3dd60c9b724e",
     "grade": false,
     "grade_id": "cell-7bbf6c13e9eaea91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell helps you build the ranking matrix for each query in the test set.\n",
    "The first row is the production ranker, the second row is the biased ranker and the third row is the unbiased ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f2a6904a62575980830a95ed5ee238d",
     "grade": false,
     "grade_id": "cell-3bdf5ac68c468d35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# we only need the ranking of the production ranker for the test set. No clicks have to be simulated here.\n",
    "test_clicks = simulate_clicks(data, 'test', click_count = 0)\n",
    "biased_predictions = get_predictions(biased_net, test_clicks, 20)\n",
    "unbiased_prediction = get_predictions(unbiased_net, test_clicks, 20)\n",
    "\n",
    "def get_ranking_matrix(qid, topk):\n",
    "    ranking_matrix = np.empty([3, min(data.test.query_size(qid), topk)], dtype=np.int32)\n",
    "    ranking = data.test.ranks[data.test.doclist_ranges[qid]:data.test.doclist_ranges[qid+1]]\n",
    "    ranking_matrix[0,:] = np.arange(ranking_matrix.shape[1])\n",
    "    ranking_matrix[1,:] = np.argsort(-biased_predictions[qid])\n",
    "    ranking_matrix[2,:] = np.argsort(-unbiased_prediction[qid])\n",
    "    return ranking_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5cc0c513947abfc38bd19f364c24992",
     "grade": false,
     "grade_id": "cell-77d4e88031772611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(get_ranking_matrix(0, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3a1de51e80c9251462e68fb16b4fb0f",
     "grade": false,
     "grade_id": "cell-ff251675418a5a1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.1 Multileaving (10 points)\n",
    "\n",
    "Given the rankings of multiple rankers, we want to decide how to fill the results list and show it to the users.\n",
    "Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1f9c735274c6a469bd089afd078bcf9",
     "grade": false,
     "grade_id": "cell-fcf5351ca3d36caa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def make_multileaving(inverted_rankings, topk):\n",
    "    '''\n",
    "    ARGS: (all np.array of docids)\n",
    "    - inverted_rankings: matrix (rankers x documents) where [x,y] corresponds to the rank of doc y in ranker x\n",
    "    RETURNS\n",
    "    - ranking of indices corresponding to inverted_rankings\n",
    "    '''\n",
    "    n_rankers = inverted_rankings.shape[0]\n",
    "    n = inverted_rankings.shape[1]\n",
    "    k = min(n, topk)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_multileaving(invert_rankings(get_ranking_matrix(0, 20)), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51819e8bd101b87700217e0ea4983c40",
     "grade": true,
     "grade_id": "cell-a7f83f82f0a41b58",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca3e1ec97c81db9c4a9e3d7ec81919c",
     "grade": false,
     "grade_id": "cell-024ab191ba5fc1c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.2: Probability of rankers (15 points)\n",
    "\n",
    "We have shown the multileaved list to the user and they clicked on some documents.\n",
    "In this function we want to calculate the probability that the clicked documents belong to a specific ranker.\n",
    "\n",
    "Note that the results of rankers are not unique and each document in the results list may belong to different rankers.\n",
    "So we need to assign a *probability* to each click belonging to each ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6150d058189bef659154cdb1e700d18",
     "grade": false,
     "grade_id": "cell-06a948902936dd40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (15 points)\n",
    "def probability_of_list(result_list, inverted_rankings, clicked_docs):\n",
    "    '''\n",
    "    ARGS: (all np.array of docids)\n",
    "    - result_list: the multileaved list\n",
    "    - inverted_rankings: matrix (rankers x documents) where [x,y] corresponds to the rank of doc y in ranker x\n",
    "    - clicked_docs: boolean array of result_list length indicating clicks\n",
    "    RETURNS\n",
    "    -sigmas: matrix (rankers x clicked_docs) with probabilty ranker added clicked doc\n",
    "    '''\n",
    "    n_docs = inverted_rankings.shape[1]\n",
    "    n_rankers = inverted_rankings.shape[0]\n",
    "\n",
    "    click_doc_ind = result_list[clicked_docs]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216c6987df2ad52d86dec05976823892",
     "grade": false,
     "grade_id": "cell-cb41d992a09ed02e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ranking_matrix = get_ranking_matrix(0, 20)\n",
    "result_list = make_multileaving(invert_rankings(ranking_matrix), 20)\n",
    "clicks = [0,4,9,10]\n",
    "probabilities = probability_of_list(result_list, invert_rankings(ranking_matrix), clicks)\n",
    "print(f'ranking matrix:\\n {ranking_matrix}')\n",
    "print(f'results list (shown to user):\\n {result_list}')\n",
    "print(f'clicked documents: {clicks}')\n",
    "print(f'probabilities:\\n {probabilities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b4921314264ee89e72d616cf8ee8f33",
     "grade": true,
     "grade_id": "cell-41eadc2f4faaf437",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cdd78a87da0a5d3a05207c5e926eef4",
     "grade": false,
     "grade_id": "cell-95c4e4ae05ce7198",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.3: Preference matrix (10 points)\n",
    "\n",
    "Given the probabilities of each ranker being clicked, we want to calculate a preference matrix that for each pair of rankers tells us which one is preferred by the clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8f2075b4b94720bdf96898d4254589",
     "grade": false,
     "grade_id": "cell-931bf5baba8e9eac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def preferences_of_list(probs):\n",
    "    '''\n",
    "    ARGS:\n",
    "    -probs: clicked docs x rankers matrix with probabilities ranker added clicked doc  (use probability_of_list)\n",
    "    -n_samples: number of samples to base preference matrix on\n",
    "    RETURNS:\n",
    "    - preference matrix: matrix (rankers x rankers) in this matrix [x,y] > 0 means x won over y and [x,y] < 0 means x lost from y\n",
    "      the value is analogous to the (average) degree of preference\n",
    "    '''\n",
    "    n_samples = 10\n",
    "    n_clicks = probs.shape[0]\n",
    "    n_rankers = probs.shape[1]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def infer_preferences(inverted_rankings, result_list, clicked_docs):\n",
    "    n_rankers = inverted_rankings.shape[0]\n",
    "    if np.any(clicked_docs):\n",
    "        return preferences_of_list(probability_of_list(result_list,\n",
    "                                        inverted_rankings,\n",
    "                                        clicked_docs))\n",
    "    else:\n",
    "        return np.zeros((n_rankers, n_rankers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2342811f833dbdbc29a5f7f5725b576",
     "grade": false,
     "grade_id": "cell-9598544608fbcde8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(preferences_of_list(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e79d7a1992af19a5fcd6711ece693800",
     "grade": true,
     "grade_id": "cell-601defbd1e14a840",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0360fb3c0a060e97efd327ec0cc79468",
     "grade": false,
     "grade_id": "cell-766b24e1ede46a8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can simulate the multileaving to see how our target rankers are evaluated by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d071088604059a588482912e2a0dfd2",
     "grade": false,
     "grade_id": "cell-45aaaa7c7a055fed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simulate_multileaving(n_impressions, topk):\n",
    "    n_samples = 10\n",
    "    impressions = 0\n",
    "    pref_update = 0\n",
    "    total_pref = np.zeros((3, 3))\n",
    "    for step_i in range(n_impressions):\n",
    "        qid = np.random.randint(0, data.test.doclist_ranges.shape[0] - 1)\n",
    "\n",
    "        start_i = data.test.doclist_ranges[qid]\n",
    "        end_i = data.test.doclist_ranges[qid + 1]\n",
    "        n_query_docs = end_i - start_i\n",
    "        query_labels = data.test.label_vector[start_i:end_i]\n",
    "\n",
    "        inverted_rankings = invert_rankings(get_ranking_matrix(qid, topk))\n",
    "        multileaving = make_multileaving(inverted_rankings, topk)\n",
    "\n",
    "        cur_clicks = generate_clicks(multileaving, query_labels)\n",
    "\n",
    "        if np.any(cur_clicks):\n",
    "            pref = infer_preferences(inverted_rankings, multileaving, cur_clicks)\n",
    "            total_pref += pref\n",
    "            pref_update += 1\n",
    "    return total_pref / pref_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95b61f7faa7cb7febe3d00fb66d0cda3",
     "grade": false,
     "grade_id": "cell-eae8a938c37a37f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "simulate_multileaving(10000, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29bb96f857fa7ae1dc2d816a1b888d7b",
     "grade": false,
     "grade_id": "cell-04ee05c85dd0304d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.4: Analysis (40 points)\n",
    "\n",
    "Analyze the behavior of your multileaving implementation by testing it with two different `topk` values: $[5,20]$ and with different number of evaluation clicks: $[2000, 10000, 50000]$.\n",
    "\n",
    "Put the preference of the unbiased method over the biased method (i.e. `pref[2,1]` in the matrix output of `simulate_multileaving` function) for these experiments in a table.\n",
    "\n",
    "**Rubric:**\n",
    "- Six experiments: 12 points\n",
    "- Analysis of the observations: 28 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd41d14078b1ad160c02cf690c45e55f",
     "grade": true,
     "grade_id": "cell-d88f4eb3035e6fbb",
     "locked": false,
     "points": 40,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
